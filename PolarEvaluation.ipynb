{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90b73ae6-1784-4327-9cb2-f999c8998aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4c2734-08db-493f-905f-9d9e8292f615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching necessary files\n",
    "\n",
    "!wget https://isic-challenge-data.s3.amazonaws.com/2018/ISIC2018_Task1-2_Training_Input.zip\n",
    "!wget https://isic-challenge-data.s3.amazonaws.com/2018/ISIC2018_Task1_Training_GroundTruth.zip\n",
    "!wget https://isic-challenge-data.s3.amazonaws.com/2018/ISIC2018_Task1-2_Validation_Input.zip\n",
    "!wget https://isic-challenge-data.s3.amazonaws.com/2018/ISIC2018_Task1_Validation_GroundTruth.zip\n",
    "\n",
    "!unzip ./ISIC2018_Task1-2_Training_Input.zip\n",
    "# !rm ./ISIC2018_Task1-2_Training_Input.zip\n",
    "!unzip ./ISIC2018_Task1_Training_GroundTruth.zip\n",
    "# !rm ./ISIC2018_Task1_Training_GroundTruth.zip\n",
    "!unzip ./ISIC2018_Task1-2_Validation_Input.zip\n",
    "# !rm ./ISIC2018_Task1-2_Validation_Input.zip\n",
    "!unzip ./ISIC2018_Task1_Validation_GroundTruth.zip\n",
    "# !rm ./ISIC2018_Task1_Validation_GroundTruth.zip\n",
    "!mkdir ./Preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "092821e2-9c0a-4b45-8474-002eca842b13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import torchinfo\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from glob import glob\n",
    "from torchvision.io import read_image\n",
    "# import scipy as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb067f3a-b8cd-42e2-b3b2-9fdc23e7c973",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir('E:\\\\Segmentation')\n",
    "\n",
    "TRAIN_INPUT_DIR = 'ISIC2018_Task1-2_Training_Input/'\n",
    "TRAIN_GT_DIR = 'ISIC2018_Task1_Training_GroundTruth/'\n",
    "TRAIN_INTERM_DIR = 'ISIC2017-Training-Interm'\n",
    "\n",
    "VAL_INPUT_DIR = 'ISIC2018_Task1-2_Validation_Input/'\n",
    "VAL_GT_DIR = 'ISIC2018_Task1_Validation_GroundTruth/'\n",
    "VAL_INTERM_DIR = 'ISIC2017-Validation-Output'\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 100\n",
    "IM_H, IM_W = 256, 256\n",
    "TRAINING_NOISE = 0\n",
    "DROPOUT = .5\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "647e5a1f-01bb-4e32-a2ee-4f31c61f14c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99e2d8e8-7c8d-453c-a172-259adc045462",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def centroid(img, lcc=False):\n",
    "  if lcc:\n",
    "    img = img.astype(np.uint8)\n",
    "    nb_components, output, stats, centroids = cv.connectedComponentsWithStats(img, connectivity=4)\n",
    "    sizes = stats[:, -1]\n",
    "    if len(sizes) > 2:\n",
    "      max_label = 1\n",
    "      max_size = sizes[1]\n",
    "\n",
    "      for i in range(2, nb_components):\n",
    "          if sizes[i] > max_size:\n",
    "              max_label = i\n",
    "              max_size = sizes[i]\n",
    "\n",
    "      img2 = np.zeros(output.shape)\n",
    "      img2[output == max_label] = 255\n",
    "      img = img2\n",
    "\n",
    "  if len(img.shape) > 2:\n",
    "    M = cv.moments(img[:,:,1])\n",
    "  else:\n",
    "    M = cv.moments(img)\n",
    "\n",
    "  if M[\"m00\"] == 0:\n",
    "    return (img.shape[0] // 2, img.shape[1] // 2)\n",
    "  \n",
    "  cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "  cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "  return (cX, cY)\n",
    "\n",
    "def to_polar(input_img, center):\n",
    "  input_img = input_img.astype(np.float32)\n",
    "  value = np.sqrt(((input_img.shape[0]/2.0)**2.0)+((input_img.shape[1]/2.0)**2.0))\n",
    "  polar_image = cv.linearPolar(input_img, center, value, cv.WARP_FILL_OUTLIERS)\n",
    "  return polar_image\n",
    "\n",
    "def to_cart(input_img, center):\n",
    "  input_img = input_img.astype(np.float32)\n",
    "  value = np.sqrt(((input_img.shape[1]/2.0)**2.0)+((input_img.shape[0]/2.0)**2.0))\n",
    "  polar_image = cv.linearPolar(input_img, center, value, cv.WARP_FILL_OUTLIERS + cv.WARP_INVERSE_MAP)\n",
    "  polar_image = polar_image.astype(np.uint8)\n",
    "  return polar_image\n",
    "\n",
    "def calc_dice(input_img, target):\n",
    "    tp = np.sum(np.minimum(input_img, target))\n",
    "    fp = np.sum(np.minimum(input_img, 1 - target))\n",
    "    fn = np.sum(np.minimum(1 - input_img, target))\n",
    "    return 2 * tp / (2 * tp + fp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bedc561-602a-470e-a65a-f2b47cf84c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISICDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_folder, seg_folder, transform=None, target_transform=None):\n",
    "        self.in_files = glob(os.path.join(input_folder, '*.jpg'))\n",
    "        self.gt_files = glob(os.path.join(seg_folder, '*.png'))\n",
    "        df = pd.read_csv('centers_new.csv')\n",
    "        self.centers = dict()\n",
    "        for item in df['file']:\n",
    "            self.centers[item] = df[df['file'] == item].to_numpy()[0,1:3].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.in_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        in_img = cv.resize(cv.imread(self.in_files[idx], cv.IMREAD_COLOR), (IM_H, IM_W))\n",
    "        in_img = cv.cvtColor(in_img, cv.COLOR_BGR2RGB)\n",
    "        # in_img = (in_img - np.mean(in_img, axis=(0,1),keepdims=True)) / np.std(in_img, axis=(0,1), keepdims=True)\n",
    "        in_img = in_img / 255.\n",
    "        gt_img = cv.resize(cv.imread(self.gt_files[idx], cv.IMREAD_GRAYSCALE), (512, 384))\n",
    "        center = self.centers[self.in_files[idx]]\n",
    "        X = to_polar(in_img, (int(center[0] * IM_H), int(center[1] * IM_W)))\n",
    "        filename = os.path.split(self.in_files[idx])[-1][:-4]\n",
    "        y = to_polar(gt_img, (int(center[0] * 512), int(center[1] * 384)))\n",
    "        mask_l = torch.tensor(to_polar(np.ones((512, 384)), center)[np.newaxis,:,:])\n",
    "        mask_s = torch.tensor(to_polar(np.ones((IM_H, IM_W)), center)[np.newaxis,:,:])\n",
    "        # print((int(center[0] * 512), int(center[1] * 384)))\n",
    "        return (X, mask_l, mask_s, y, filename, center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5334fd82-5a55-425f-acd0-f8cba4d003cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISICDatasetTrain(torch.utils.data.Dataset):\n",
    "    def __init__(self, src):\n",
    "        self.src = src\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X, mask_l, mask_s, y, filename, center = self.src[idx]\n",
    "        X = torch.tensor(X).transpose(2, 1).transpose(1, 0) / 255.\n",
    "        y = cv.resize(y, (IM_H, IM_W))[np.newaxis,:,:]\n",
    "        y = torch.tensor(y) / 255.\n",
    "        y = torch.round(y)\n",
    "        return (X, mask_l, mask_s, y, filename, center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd86fa7b-2b83-43ba-bb17-1785bc1af54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISICDatasetTest(torch.utils.data.Dataset):\n",
    "    def __init__(self, src):\n",
    "        self.src = src\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X, mask_l, mask_s, y, filename, center = self.src[idx]\n",
    "        X = torch.tensor(X).transpose(2, 1).transpose(1, 0) / 255.\n",
    "        y = y[np.newaxis,:,:]\n",
    "        y = torch.tensor(y) // 255.\n",
    "        y = torch.round(y)\n",
    "        return (X, mask_l, mask_s, y, filename, center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47f97d07-0abd-49c6-8089-954ba6e7721d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2594 2594\n"
     ]
    }
   ],
   "source": [
    "full_ds = ISICDataset(TRAIN_INPUT_DIR, TRAIN_GT_DIR)\n",
    "n, nt = len(full_ds), int(len(full_ds) / 10)\n",
    "train_ds_p, valid_ds_p, test_ds_p = random_split(full_ds, [n - 2 * nt, nt, nt])\n",
    "train_ds = ISICDatasetTrain(train_ds_p)\n",
    "valid_ds = ISICDatasetTrain(valid_ds_p)\n",
    "test_ds = ISICDatasetTest(test_ds_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f26e677-d76d-4df6-9bea-257ed3bc111e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, pin_memory=False)\n",
    "valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=True, pin_memory=False)\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=1, shuffle=False, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95398bfa-2f90-4028-bc01-87592014b6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with mask\n",
    "\n",
    "def train(model):\n",
    "    num_batches = len(train_dl)\n",
    "    model.train()\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    dices = []\n",
    "    with tqdm(total=num_batches) as pbar:\n",
    "        pbar.set_description(\"Avg.Loss: 0.0000, Avg. Accuracy: 0.0000\")\n",
    "        for (X, mask_l, mask_s, y, filename, center) in train_dl:\n",
    "            loss, met = model.fit(X, mask_s, y)\n",
    "            losses.append(loss)\n",
    "            accuracies.append((met[0] + met[3]) / (met[0] + met[1] + met[2] + met[3]))\n",
    "            dices.append(met[0] * 2 / (met[0] * 2 + met[1] + met[2]))\n",
    "            pbar.update(1)\n",
    "            pbar.set_description(f\"Avg. Loss: {torch.mean(torch.tensor(losses)):.4f}, Avg. Accuracy: {torch.mean(torch.tensor(accuracies)):.4f}, Avg. Dice: {torch.mean(torch.tensor(dices)):.4f}\")\n",
    "    return torch.mean(torch.tensor(losses)).item(), torch.mean(torch.tensor(dices)).item()\n",
    "\n",
    "def test(model):\n",
    "    num_batches = len(valid_dl)\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    dices = []\n",
    "    with tqdm(total=num_batches) as pbar:\n",
    "        pbar.set_description(\"Avg.Loss: 0.0000, Avg. Accuracy: 0.0000\")\n",
    "        with torch.no_grad():\n",
    "            for (X, mask_l, mask_s, y, filename, center) in valid_dl:\n",
    "                loss, met = model.test(X, mask_s, y)\n",
    "                losses.append(loss)\n",
    "                accuracies.append((met[0] + met[3]) / (met[0] + met[1] + met[2] + met[3]))\n",
    "                dices.append(met[0] * 2 / (met[0] * 2 + met[1] + met[2]))\n",
    "                pbar.update(1)\n",
    "                pbar.set_description(f\"Avg. Loss: {torch.mean(torch.tensor(losses)):.4f}, Avg. Accuracy: {torch.mean(torch.tensor(accuracies)):.4f}, Avg. Dice: {torch.mean(torch.tensor(dices)):.4f}\")\n",
    "    return torch.mean(torch.tensor(losses)).item(), torch.mean(torch.tensor(dices)).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d654ca-fc80-468d-971b-a28b4d35857e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e6304b3-9a04-4ad7-84c5-2231a967ffb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_loss(pred, mask):\n",
    "    weit = 1 + 5 * torch.abs(F.avg_pool2d(mask, kernel_size=31, stride=1, padding=15) - mask)\n",
    "    \n",
    "    pred = torch.sigmoid(pred)\n",
    "    inter = ((pred * mask) * weit).sum(dim=(2, 3))\n",
    "    union = ((pred + mask) * weit).sum(dim=(2, 3))\n",
    "    iou = 1 - (inter + 1) / (union - inter + 1)\n",
    "\n",
    "    return iou.mean()\n",
    "\n",
    "def dice_loss(pred, mask, cover):\n",
    "    weit = 1 # + 5 * torch.abs(F.avg_pool2d(mask, kernel_size=7, stride=1, padding=3) - mask)\n",
    "    \n",
    "    pred = torch.sigmoid(pred)\n",
    "    tp = ((pred * mask) * weit).sum(dim=(2, 3))\n",
    "    fp = ((pred * (1 - mask)) * weit).sum(dim=(2, 3))\n",
    "    fn = (((1 - pred) * mask) * weit).sum(dim=(2, 3))\n",
    "    dice = 1 - (2 * tp + 1) / (2 * tp + fp + fn + 1)\n",
    "\n",
    "    return dice.mean()\n",
    "\n",
    "class FocalLoss():\n",
    "    def __init__(self, gamma):\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def __call__(self, pred, mask):\n",
    "        pred = torch.sigmoid(pred)\n",
    "        dist = torch.abs(pred - mask)\n",
    "        bce = -torch.log(1 - dist)\n",
    "        focal_adjusted = torch.mean(dist**self.gamma * bce)\n",
    "        return focal_adjusted\n",
    "\n",
    "def focal_struct_loss(pred, mask):\n",
    "    weit = torch.abs(mask - F.sigmoid(pred))\n",
    "    wbce = F.binary_cross_entropy_with_logits(pred, mask, reduce='none')\n",
    "    wbce = (weit * wbce).sum(dim=(2, 3)) / weit.sum(dim=(2, 3))\n",
    "\n",
    "    pred = torch.sigmoid(pred)\n",
    "    inter = ((pred * mask) * weit).sum(dim=(2, 3))\n",
    "    union = ((pred + mask) * weit).sum(dim=(2, 3))\n",
    "    wiou = 1 - (inter + 1) / (union - inter + 1)\n",
    "\n",
    "    return (wbce + wiou).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "901a9720-6597-4f0b-afdc-41b040987e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from lib.pvtv2 import pvt_v2_b2\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from mmcv.cnn import ConvModule\n",
    "from torch.nn import Conv2d, UpsamplingBilinear2d\n",
    "import warnings\n",
    "import torch\n",
    "# from mmcv.cnn import constant_init, kaiming_init\n",
    "from torch import nn\n",
    "from torchvision.transforms.functional import normalize\n",
    "from PVTv2.pvtv2 import pvt_v2_b2\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SpatBlock(nn.Module):\n",
    "    def __init__(self, dim=32, order=5):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.order = order\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        self.blocks = nn.ModuleList([nn.Conv2d(1, dim, 1)] + [nn.Conv2d(1, dim, 1, bias=False) for _ in range(order-1)])\n",
    "\n",
    "        self = self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        pos = torch.arange(W) / W\n",
    "        pos = torch.reshape(pos, (1, 1, 1, W)).to(self.device)\n",
    "        \n",
    "        func = torch.zeros_like(x)\n",
    "\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            func += block(pos**(i+1))\n",
    "\n",
    "        out = x + func\n",
    "        \n",
    "        return out\n",
    "        \n",
    "\n",
    "def structure_loss(pred, mask):\n",
    "    weit = 1 + 5 * torch.abs(F.avg_pool2d(mask, kernel_size=31, stride=1, padding=15) - mask)\n",
    "    wbce = F.binary_cross_entropy_with_logits(pred, mask, reduce='none')\n",
    "    wbce = (weit * wbce).sum(dim=(2, 3)) / weit.sum(dim=(2, 3))\n",
    "\n",
    "    pred = torch.sigmoid(pred)\n",
    "    inter = ((pred * mask) * weit).sum(dim=(2, 3))\n",
    "    union = ((pred + mask) * weit).sum(dim=(2, 3))\n",
    "    wiou = 1 - (inter + 1) / (union - inter + 1)\n",
    "\n",
    "    return (wbce + wiou).mean()\n",
    "\n",
    "# def structure_loss(pred, mask):\n",
    "#     pred = torch.sigmoid(pred)\n",
    "#     loss1 = F.binary_cross_entropy(pred, mask)\n",
    "#     tp = pred * mask\n",
    "#     fp = pred * (1 - mask)\n",
    "#     fn = (1 - pred) * mask\n",
    "#     tp = torch.mean(tp, dim=0)\n",
    "#     fp = torch.mean(fp, dim=0)\n",
    "#     fn = torch.mean(fn, dim=0)\n",
    "#     iou = tp / (tp + fp + fn)\n",
    "#     iou = torch.mean(iou)\n",
    "#     loss2 = 1 - iou\n",
    "#     return loss1 + loss2\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes,\n",
    "                              kernel_size=kernel_size, stride=stride,\n",
    "                              padding=padding, dilation=dilation, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "        \n",
    "class Block(nn.Sequential):\n",
    "    def __init__(self, input_num, num1, num2, dilation_rate, drop_out, bn_start=True, norm_layer=nn.BatchNorm2d):\n",
    "        super(Block, self).__init__()\n",
    "        if bn_start:\n",
    "            self.add_module('norm1', norm_layer(input_num)),\n",
    "\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv1', nn.Conv2d(in_channels=input_num, out_channels=num1, kernel_size=1)),\n",
    "\n",
    "        self.add_module('norm2', norm_layer(num1)),\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv2', nn.Conv2d(in_channels=num1, out_channels=num2, kernel_size=3,\n",
    "                                            dilation=dilation_rate, padding=dilation_rate)),\n",
    "        self.drop_rate = drop_out\n",
    "\n",
    "    def forward(self, _input):\n",
    "        feature = super(Block, self).forward(_input)\n",
    "        if self.drop_rate > 0:\n",
    "            feature = F.dropout2d(feature, p=self.drop_rate, training=self.training)\n",
    "        return feature\n",
    "\n",
    "\n",
    "def Upsample(x, size, align_corners = False):\n",
    "    \"\"\"\n",
    "    Wrapper Around the Upsample Call\n",
    "    \"\"\"\n",
    "    return nn.functional.interpolate(x, size=size, mode='bilinear', align_corners=align_corners)\n",
    "\n",
    "    \n",
    "# def last_zero_init(m):\n",
    "#     if isinstance(m, nn.Sequential):\n",
    "#         constant_init(m[-1], val=0)\n",
    "#     else:\n",
    "#         constant_init(m, val=0)\n",
    "\n",
    "\n",
    "class ContextBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 inplanes,\n",
    "                 ratio,\n",
    "                 pooling_type='att',\n",
    "                 fusion_types=('channel_mul', )):\n",
    "        super(ContextBlock, self).__init__()\n",
    "        assert pooling_type in ['avg', 'att']\n",
    "        assert isinstance(fusion_types, (list, tuple))\n",
    "        valid_fusion_types = ['channel_add', 'channel_mul']\n",
    "        assert all([f in valid_fusion_types for f in fusion_types])\n",
    "        assert len(fusion_types) > 0, 'at least one fusion should be used'\n",
    "        self.inplanes = inplanes\n",
    "        self.ratio = ratio\n",
    "        self.planes = int(inplanes * ratio)\n",
    "        self.pooling_type = pooling_type\n",
    "        self.fusion_types = fusion_types\n",
    "        if pooling_type == 'att':\n",
    "            self.conv_mask = nn.Conv2d(inplanes, 1, kernel_size=1)\n",
    "            self.softmax = nn.Softmax(dim=2)\n",
    "        else:\n",
    "            self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        if 'channel_add' in fusion_types:\n",
    "            self.channel_add_conv = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, self.planes, kernel_size=1),\n",
    "                nn.LayerNorm([self.planes, 1, 1]),\n",
    "                nn.ReLU(inplace=True),  # yapf: disable\n",
    "                nn.Conv2d(self.planes, self.inplanes, kernel_size=1))\n",
    "        else:\n",
    "            self.channel_add_conv = None\n",
    "        if 'channel_mul' in fusion_types:\n",
    "            self.channel_mul_conv = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, self.planes, kernel_size=1),\n",
    "                nn.LayerNorm([self.planes, 1, 1]),\n",
    "                nn.ReLU(inplace=True),  # yapf: disable\n",
    "                nn.Conv2d(self.planes, self.inplanes, kernel_size=1))\n",
    "        else:\n",
    "            self.channel_mul_conv = None\n",
    "\n",
    "    def spatial_pool(self, x):\n",
    "        batch, channel, height, width = x.size()\n",
    "        if self.pooling_type == 'att':\n",
    "            input_x = x\n",
    "            # [N, C, H * W]\n",
    "            input_x = input_x.view(batch, channel, height * width)\n",
    "            # [N, 1, C, H * W]\n",
    "            input_x = input_x.unsqueeze(1)\n",
    "            # [N, 1, H, W]\n",
    "            context_mask = self.conv_mask(x)\n",
    "            # [N, 1, H * W]\n",
    "            context_mask = context_mask.view(batch, 1, height * width)\n",
    "            # [N, 1, H * W]\n",
    "            context_mask = self.softmax(context_mask)\n",
    "            # [N, 1, H * W, 1]\n",
    "            context_mask = context_mask.unsqueeze(-1)\n",
    "            # [N, 1, C, 1]\n",
    "            context = torch.matmul(input_x, context_mask)\n",
    "            # [N, C, 1, 1]\n",
    "            context = context.view(batch, channel, 1, 1)\n",
    "        else:\n",
    "            # [N, C, 1, 1]\n",
    "            context = self.avg_pool(x)\n",
    "\n",
    "        return context\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [N, C, 1, 1]\n",
    "        context = self.spatial_pool(x)\n",
    "\n",
    "        out = x\n",
    "        if self.channel_mul_conv is not None:\n",
    "            # [N, C, 1, 1]\n",
    "            channel_mul_term = torch.sigmoid(self.channel_mul_conv(context))\n",
    "            out = out + out * channel_mul_term\n",
    "        if self.channel_add_conv is not None:\n",
    "            # [N, C, 1, 1]\n",
    "            channel_add_term = self.channel_add_conv(context)\n",
    "            out = out + channel_add_term\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1   = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2   = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "\n",
    "class ConvBranch(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features = None, out_features = None):\n",
    "        super().__init__()\n",
    "        hidden_features = hidden_features or in_features\n",
    "        out_features = out_features or in_features\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_features, hidden_features, 1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, hidden_features, 3, padding=1, groups=hidden_features, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, hidden_features, 1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, hidden_features, 3, padding=1, groups=hidden_features, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, hidden_features, 1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.SiLU(inplace=True)\n",
    "        )\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, hidden_features, 3, padding=1, groups=hidden_features, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv7 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, out_features, 1, bias=False),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.ca = ChannelAttention(64)\n",
    "        self.sa = SpatialAttention()\n",
    "        self.sigmoid_spatial = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        res1 = x\n",
    "        res2 = x\n",
    "        x = self.conv1(x)        \n",
    "        x = x + self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x + self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = x + self.conv6(x)\n",
    "        x = self.conv7(x)\n",
    "        x_mask = self.sigmoid_spatial(x)\n",
    "        res1 = res1 * x_mask\n",
    "        return res2 + res1\n",
    "\n",
    "              \n",
    "class GLSA(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=512, embed_dim=32, k_s=3):\n",
    "        super().__init__()\n",
    "                      \n",
    "        self.conv1_1 = BasicConv2d(embed_dim*2,embed_dim, 1)\n",
    "        self.conv1_1_1 = BasicConv2d(input_dim//2,embed_dim,1)\n",
    "        self.local_11conv = nn.Conv2d(input_dim//2,embed_dim,1)\n",
    "        self.global_11conv = nn.Conv2d(input_dim//2,embed_dim,1)\n",
    "        self.GlobelBlock = ContextBlock(inplanes= embed_dim, ratio=2)\n",
    "        self.local = ConvBranch(in_features = embed_dim, hidden_features = embed_dim, out_features = embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "        x_0, x_1 = x.chunk(2,dim = 1)  \n",
    "        \n",
    "    # local block \n",
    "        local = self.local(self.local_11conv(x_0))\n",
    "        \n",
    "    # Globel block    \n",
    "        Globel = self.GlobelBlock(self.global_11conv(x_1))\n",
    "\n",
    "    # concat Globel + local\n",
    "        x = torch.cat([local,Globel], dim=1)\n",
    "        x = self.conv1_1(x)\n",
    "\n",
    "        return x    \n",
    "\n",
    "class SBA(nn.Module):\n",
    "\n",
    "    def __init__(self,input_dim = 64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.d_in1 = BasicConv2d(input_dim//2, input_dim//2, 1)\n",
    "        self.d_in2 = BasicConv2d(input_dim//2, input_dim//2, 1)       \n",
    "                \n",
    "\n",
    "        self.conv = nn.Sequential(BasicConv2d(input_dim, input_dim, 3,1,1), nn.Conv2d(input_dim, 1, kernel_size=1, bias=False))\n",
    "        self.fc1 = nn.Conv2d(input_dim, input_dim//2, kernel_size=1, bias=False)\n",
    "        self.fc2 = nn.Conv2d(input_dim, input_dim//2, kernel_size=1, bias=False)\n",
    "        \n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, H_feature, L_feature):\n",
    "\n",
    "        L_feature = self.fc1(L_feature)\n",
    "        H_feature = self.fc2(H_feature)\n",
    "        \n",
    "        g_L_feature =  self.Sigmoid(L_feature)\n",
    "        g_H_feature = self.Sigmoid(H_feature)\n",
    "        \n",
    "        L_feature = self.d_in1(L_feature)\n",
    "        H_feature = self.d_in2(H_feature)\n",
    "\n",
    "\n",
    "        L_feature = L_feature + L_feature * g_L_feature + (1 - g_L_feature) * Upsample(g_H_feature * H_feature, size= L_feature.size()[2:], align_corners=False)\n",
    "        H_feature = H_feature + H_feature * g_H_feature + (1 - g_H_feature) * Upsample(g_L_feature * L_feature, size= H_feature.size()[2:], align_corners=False) \n",
    "        \n",
    "        H_feature = Upsample(H_feature, size = L_feature.size()[2:])\n",
    "        out = self.conv(torch.cat([H_feature,L_feature], dim=1))\n",
    "        return out\n",
    "        \n",
    "            \n",
    "class DuAT(nn.Module):\n",
    "    def __init__(self, dim=32, dims= [64, 256, 512, 1024], learning_rate=None,  loss_fn=None, optimizer=None, device=None, weight_decay=None):\n",
    "        super(DuAT, self).__init__()\n",
    "\n",
    "                    \n",
    "        if (device is None):\n",
    "            self.device = DEVICE\n",
    "        else:\n",
    "            self.device = device\n",
    "\n",
    "        if (learning_rate is None):\n",
    "            self.learning_rate = 1e-5\n",
    "        else:\n",
    "            self.learning_rate = learning_rate\n",
    "        \n",
    "        if (weight_decay is None):\n",
    "            self.weight_decay = 1e-5\n",
    "        else:\n",
    "            self.weight_decay = weight_decay\n",
    "\n",
    "        self.dims = dims\n",
    "\n",
    "        # self.pad = False\n",
    "        resnet50 = torchvision.models.resnet50(weights=\"DEFAULT\")\n",
    "        self.e1 = nn.Sequential(resnet50._modules['conv1'], resnet50._modules['bn1'], resnet50._modules['relu'])\n",
    "        self.e2 = nn.Sequential(resnet50._modules['maxpool'], resnet50._modules['layer1'])\n",
    "        self.e3 = nn.Sequential(resnet50._modules['layer2'])\n",
    "        self.e4 = nn.Sequential(resnet50._modules['layer3'])\n",
    "        \n",
    "        c1_in_channels, c2_in_channels, c3_in_channels, c4_in_channels = dims[0], dims[1], dims[2], dims[3]\n",
    "\n",
    "        self.shuffle4 = nn.Conv2d(c4_in_channels, c4_in_channels, 1)\n",
    "        self.shuffle3 = nn.Conv2d(c3_in_channels, c3_in_channels, 1)\n",
    "        self.shuffle2 = nn.Conv2d(c2_in_channels, c2_in_channels, 1)\n",
    "        self.shuffle1 = nn.Conv2d(c1_in_channels, c1_in_channels, 1)\n",
    "\n",
    "        self.bn4 = nn.BatchNorm2d(c4_in_channels)\n",
    "        self.bn3 = nn.BatchNorm2d(c3_in_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(c2_in_channels)\n",
    "        self.bn1 = nn.BatchNorm2d(c1_in_channels)\n",
    "\n",
    "        self.embed4 = SpatBlock(dim=c4_in_channels)\n",
    "        self.embed3 = SpatBlock(dim=c3_in_channels)\n",
    "        self.embed2 = SpatBlock(dim=c2_in_channels)\n",
    "        self.embed1 = SpatBlock(dim=c1_in_channels)\n",
    "\n",
    "        self.bn4 = nn.BatchNorm2d(c4_in_channels)\n",
    "        self.bn3 = nn.BatchNorm2d(c3_in_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(c2_in_channels)\n",
    "        self.bn1 = nn.BatchNorm2d(c1_in_channels)\n",
    "\n",
    "        self.GLSA_c4 = GLSA(input_dim=c4_in_channels, embed_dim=dim)\n",
    "        self.GLSA_c3 = GLSA(input_dim=c3_in_channels, embed_dim=dim)\n",
    "        self.GLSA_c2 = GLSA(input_dim=c2_in_channels, embed_dim=dim)\n",
    "        self.L_feature = BasicConv2d(c1_in_channels, dim, 3,1,1)\n",
    "        \n",
    "        self.SBA = SBA(input_dim = dim)\n",
    "        self.fuse = BasicConv2d(dim * 2, dim, 1)\n",
    "        self.fuse2 = nn.Sequential(BasicConv2d(dim*3, dim, 1,1),nn.BatchNorm2d(dim),nn.Conv2d(dim, 1, kernel_size=1, bias=False))\n",
    "\n",
    "\n",
    "        for param in self.e1.parameters():\n",
    "            param.requires_grad_(False)\n",
    "            \n",
    "        for param in self.e2.parameters():\n",
    "            param.requires_grad_(False)\n",
    "            \n",
    "        for param in self.e3.parameters():\n",
    "            param.requires_grad_(False)\n",
    "            \n",
    "        for param in self.e4.parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "        if (loss_fn is None):\n",
    "            self.loss_fn = structure_loss\n",
    "        else:\n",
    "            self.loss_fn = loss_fn\n",
    "\n",
    "        if (optimizer is None):\n",
    "            self.optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        else:\n",
    "            self.optimizer = optimizer(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "\n",
    "        self = self.to(self.device)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # backbone\n",
    "        x = x.to(self.device)\n",
    "        c1 = self.e1(x)\n",
    "        c2 = self.e2(c1)\n",
    "        c3 = self.e3(c2)\n",
    "        c4 = self.e4(c3)\n",
    "\n",
    "\n",
    "        \n",
    "        # c1 = self.bn1(self.shuffle1(c1))\n",
    "        # c2 = self.bn2(self.shuffle2(c2))\n",
    "        # c3 = self.bn3(self.shuffle3(c3))\n",
    "        # c4 = self.bn4(self.shuffle4(c4))\n",
    "        \n",
    "        # c1 = torch.cat([c1,\n",
    "        #                 torch.arange(c1.shape[2], dtype=c1.dtype, device=self.device).view(1,1,-1,1).repeat(c1.shape[0], 1, 1, c1.shape[3]) / c1.shape[2],\n",
    "        #                 torch.arange(c1.shape[3], dtype=c1.dtype, device=self.device).view(1,1,1,-1).repeat(c1.shape[0], 1, c1.shape[2], 1) / c1.shape[3]], dim=1)\n",
    "        # c2 = torch.cat([c2,\n",
    "        #                 torch.arange(c2.shape[2], dtype=c2.dtype, device=self.device).view(1,1,-1,1).repeat(c2.shape[0], 1, 1, c2.shape[3]) / c2.shape[2],\n",
    "        #                 torch.arange(c2.shape[3], dtype=c2.dtype, device=self.device).view(1,1,1,-1).repeat(c2.shape[0], 1, c2.shape[2], 1) / c2.shape[3]], dim=1)\n",
    "        # c3 = torch.cat([c3,\n",
    "        #                 torch.arange(c3.shape[2], dtype=c3.dtype, device=self.device).view(1,1,-1,1).repeat(c1.shape[0], 1, 1, c3.shape[3]) / c3.shape[2],\n",
    "        #                 torch.arange(c3.shape[3], dtype=c3.dtype, device=self.device).view(1,1,1,-1).repeat(c1.shape[0], 1, c3.shape[2], 1) / c3.shape[3]], dim=1)\n",
    "        # c4 = torch.cat([c4,\n",
    "        #                 torch.arange(c4.shape[2], dtype=c4.dtype, device=self.device).view(1,1,-1,1).repeat(c4.shape[0], 1, 1, c4.shape[3]) / c4.shape[2],\n",
    "        #                 torch.arange(c4.shape[3], dtype=c4.dtype, device=self.device).view(1,1,1,-1).repeat(c4.shape[0], 1, c4.shape[2], 1) / c4.shape[3]], dim=1)\n",
    "\n",
    "        c1 = self.bn1(self.embed1(c1))\n",
    "        c2 = self.bn2(self.embed2(c2))\n",
    "        c3 = self.bn3(self.embed3(c3))\n",
    "        c4 = self.bn4(self.embed4(c4))\n",
    "        \n",
    "        n, _, h, w = c4.shape        \n",
    "        _c4 = self.GLSA_c4(c4) # [1, 64, 11, 11]\n",
    "        _c4 = Upsample(_c4, c3.size()[2:])\n",
    "        _c3 = self.GLSA_c3(c3) # [1, 64, 22, 22]\n",
    "        _c2 = self.GLSA_c2(c2) # [1, 64, 44, 44]\n",
    "        \n",
    "        output = torch.cat([Upsample(_c4, c2.size()[2:]), Upsample(_c3, c2.size()[2:]), _c2], dim=1)\n",
    "        \n",
    "        L_feature = self.L_feature(c1)  # [1, 64, 88, 88]\n",
    "        H_feature = self.fuse(torch.cat([_c4, _c3], dim=1))\n",
    "        H_feature = Upsample(H_feature,c2.size()[2:])\n",
    "        \n",
    "        output2 = self.SBA(H_feature,L_feature)\n",
    "        h = x.shape[2] // 4\n",
    "        output = F.interpolate(output, scale_factor=4, mode='bicubic')\n",
    "        # output = torch.cat([output, Upsample(c1, output.size()[2:])], dim=1)\n",
    "        output = self.fuse2(output)\n",
    "        # return torch.sigmoid(output)\n",
    "        output2 = F.interpolate(output2, scale_factor=2, mode='bicubic')\n",
    "\n",
    "        \n",
    "        return output, output2\n",
    "        # return F.sigmoid(output[:,:,h:-h,:])\n",
    "        \n",
    "    def fit(self, X, mask, y):\n",
    "        X, mask, y = X.to(self.device), mask.to(self.device), y.to(self.device)\n",
    "        h1, h2 = self(X, mask)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.loss_fn(h1, y) + self.loss_fn(h2, y)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        loss = loss.item()\n",
    "        pred = (h2 > 0).int()\n",
    "        numt = torch.sum(mask)\n",
    "        TP = torch.sum(torch.minimum(y, pred)).item() / numt\n",
    "        TN = torch.sum(torch.minimum(1-y, 1-pred) * mask).item() / numt\n",
    "        FN = torch.sum(torch.minimum(y, 1-pred)).item() / numt\n",
    "        FP = torch.sum(torch.minimum(1-y, pred)).item() / numt\n",
    "        return (loss, torch.tensor([TP, FP, FN, TN]))\n",
    "\n",
    "    def test(self, X, mask, y):\n",
    "        X, mask, y = X.to(self.device), mask.to(self.device), y.to(self.device)\n",
    "        h1, h2 = self(X, mask)\n",
    "        loss = self.loss_fn(h1, y) + self.loss_fn(h2, y)\n",
    "        loss = loss.item()\n",
    "        pred = (h2 > 0).int()\n",
    "        numt = torch.sum(mask)\n",
    "        TP = torch.sum(torch.minimum(y, pred)).item() / numt\n",
    "        TN = torch.sum(torch.minimum(1-y, 1-pred) * mask).item() / numt\n",
    "        FN = torch.sum(torch.minimum(y, 1-pred)).item() / numt\n",
    "        FP = torch.sum(torch.minimum(1-y, pred)).item() / numt\n",
    "        return (loss, torch.tensor([TP, FP, FN, TN]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5046e202-6b2e-4249-9c77-dd84b64fce2f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# PVT DuAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afa73aee-4094-4373-a67f-e997b4b21222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from lib.pvtv2 import pvt_v2_b2\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from mmcv.cnn import ConvModule\n",
    "from torch.nn import Conv2d, UpsamplingBilinear2d\n",
    "import warnings\n",
    "import torch\n",
    "# from mmcv.cnn import constant_init, kaiming_init\n",
    "from torch import nn\n",
    "from torchvision.transforms.functional import normalize\n",
    "from BaseModel import BaseModel\n",
    "from PVT.segmentation.pvt import pvt_large, pvt_medium, pvt_small, pvt_tiny\n",
    "from PVTv2.pvtv2 import *\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def structure_loss(pred, mask):\n",
    "    weit = 1 + 5 * torch.abs(F.avg_pool2d(mask, kernel_size=31, stride=1, padding=15) - mask)\n",
    "    wbce = F.binary_cross_entropy_with_logits(pred, mask, reduce='none')\n",
    "    wbce = (weit * wbce).sum(dim=(2, 3)) / weit.sum(dim=(2, 3))\n",
    "\n",
    "    pred = torch.sigmoid(pred)\n",
    "    inter = ((pred * mask) * weit).sum(dim=(2, 3))\n",
    "    union = ((pred + mask) * weit).sum(dim=(2, 3))\n",
    "    wiou = 1 - (inter + 1) / (union - inter + 1)\n",
    "\n",
    "    return (wbce + wiou).mean()\n",
    "\n",
    "# def structure_loss(pred, mask):\n",
    "#     pred = torch.sigmoid(pred)\n",
    "#     loss1 = F.binary_cross_entropy(pred, mask)\n",
    "#     tp = pred * mask\n",
    "#     fp = pred * (1 - mask)\n",
    "#     fn = (1 - pred) * mask\n",
    "#     tp = torch.mean(tp, dim=0)\n",
    "#     fp = torch.mean(fp, dim=0)\n",
    "#     fn = torch.mean(fn, dim=0)\n",
    "#     iou = tp / (tp + fp + fn)\n",
    "#     iou = torch.mean(iou)\n",
    "#     loss2 = 1 - iou\n",
    "#     return loss1 + loss2\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes,\n",
    "                              kernel_size=kernel_size, stride=stride,\n",
    "                              padding=padding, dilation=dilation, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "        \n",
    "class Block(nn.Sequential):\n",
    "    def __init__(self, input_num, num1, num2, dilation_rate, drop_out, bn_start=True, norm_layer=nn.BatchNorm2d):\n",
    "        super(Block, self).__init__()\n",
    "        if bn_start:\n",
    "            self.add_module('norm1', norm_layer(input_num)),\n",
    "\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv1', nn.Conv2d(in_channels=input_num, out_channels=num1, kernel_size=1)),\n",
    "\n",
    "        self.add_module('norm2', norm_layer(num1)),\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv2', nn.Conv2d(in_channels=num1, out_channels=num2, kernel_size=3,\n",
    "                                            dilation=dilation_rate, padding=dilation_rate)),\n",
    "        self.drop_rate = drop_out\n",
    "\n",
    "    def forward(self, _input):\n",
    "        feature = super(Block, self).forward(_input)\n",
    "        if self.drop_rate > 0:\n",
    "            feature = F.dropout2d(feature, p=self.drop_rate, training=self.training)\n",
    "        return feature\n",
    "\n",
    "\n",
    "def Upsample(x, size, align_corners = False):\n",
    "    \"\"\"\n",
    "    Wrapper Around the Upsample Call\n",
    "    \"\"\"\n",
    "    return nn.functional.interpolate(x, size=size, mode='bilinear', align_corners=align_corners)\n",
    "\n",
    "    \n",
    "# def last_zero_init(m):\n",
    "#     if isinstance(m, nn.Sequential):\n",
    "#         constant_init(m[-1], val=0)\n",
    "#     else:\n",
    "#         constant_init(m, val=0)\n",
    "\n",
    "\n",
    "class ContextBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 inplanes,\n",
    "                 ratio,\n",
    "                 pooling_type='att',\n",
    "                 fusion_types=('channel_mul', )):\n",
    "        super(ContextBlock, self).__init__()\n",
    "        assert pooling_type in ['avg', 'att']\n",
    "        assert isinstance(fusion_types, (list, tuple))\n",
    "        valid_fusion_types = ['channel_add', 'channel_mul']\n",
    "        assert all([f in valid_fusion_types for f in fusion_types])\n",
    "        assert len(fusion_types) > 0, 'at least one fusion should be used'\n",
    "        self.inplanes = inplanes\n",
    "        self.ratio = ratio\n",
    "        self.planes = int(inplanes * ratio)\n",
    "        self.pooling_type = pooling_type\n",
    "        self.fusion_types = fusion_types\n",
    "        if pooling_type == 'att':\n",
    "            self.conv_mask = nn.Conv2d(inplanes, 1, kernel_size=1)\n",
    "            self.softmax = nn.Softmax(dim=2)\n",
    "        else:\n",
    "            self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        if 'channel_add' in fusion_types:\n",
    "            self.channel_add_conv = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, self.planes, kernel_size=1),\n",
    "                nn.LayerNorm([self.planes, 1, 1]),\n",
    "                nn.ReLU(inplace=True),  # yapf: disable\n",
    "                nn.Conv2d(self.planes, self.inplanes, kernel_size=1))\n",
    "        else:\n",
    "            self.channel_add_conv = None\n",
    "        if 'channel_mul' in fusion_types:\n",
    "            self.channel_mul_conv = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, self.planes, kernel_size=1),\n",
    "                nn.LayerNorm([self.planes, 1, 1]),\n",
    "                nn.ReLU(inplace=True),  # yapf: disable\n",
    "                nn.Conv2d(self.planes, self.inplanes, kernel_size=1))\n",
    "        else:\n",
    "            self.channel_mul_conv = None\n",
    "\n",
    "    def spatial_pool(self, x):\n",
    "        batch, channel, height, width = x.size()\n",
    "        if self.pooling_type == 'att':\n",
    "            input_x = x\n",
    "            # [N, C, H * W]\n",
    "            input_x = input_x.view(batch, channel, height * width)\n",
    "            # [N, 1, C, H * W]\n",
    "            input_x = input_x.unsqueeze(1)\n",
    "            # [N, 1, H, W]\n",
    "            context_mask = self.conv_mask(x)\n",
    "            # [N, 1, H * W]\n",
    "            context_mask = context_mask.view(batch, 1, height * width)\n",
    "            # [N, 1, H * W]\n",
    "            context_mask = self.softmax(context_mask)\n",
    "            # [N, 1, H * W, 1]\n",
    "            context_mask = context_mask.unsqueeze(-1)\n",
    "            # [N, 1, C, 1]\n",
    "            context = torch.matmul(input_x, context_mask)\n",
    "            # [N, C, 1, 1]\n",
    "            context = context.view(batch, channel, 1, 1)\n",
    "        else:\n",
    "            # [N, C, 1, 1]\n",
    "            context = self.avg_pool(x)\n",
    "\n",
    "        return context\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [N, C, 1, 1]\n",
    "        context = self.spatial_pool(x)\n",
    "\n",
    "        out = x\n",
    "        if self.channel_mul_conv is not None:\n",
    "            # [N, C, 1, 1]\n",
    "            channel_mul_term = torch.sigmoid(self.channel_mul_conv(context))\n",
    "            out = out + out * channel_mul_term\n",
    "        if self.channel_add_conv is not None:\n",
    "            # [N, C, 1, 1]\n",
    "            channel_add_term = self.channel_add_conv(context)\n",
    "            out = out + channel_add_term\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1   = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2   = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "\n",
    "class ConvBranch(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features = None, out_features = None):\n",
    "        super().__init__()\n",
    "        hidden_features = hidden_features or in_features\n",
    "        out_features = out_features or in_features\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_features, hidden_features, 1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, hidden_features, 3, padding=1, groups=hidden_features, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, hidden_features, 1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, hidden_features, 3, padding=1, groups=hidden_features, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, hidden_features, 1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.SiLU(inplace=True)\n",
    "        )\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, hidden_features, 3, padding=1, groups=hidden_features, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv7 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, out_features, 1, bias=False),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.ca = ChannelAttention(64)\n",
    "        self.sa = SpatialAttention()\n",
    "        self.sigmoid_spatial = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        res1 = x\n",
    "        res2 = x\n",
    "        x = self.conv1(x)        \n",
    "        x = x + self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x + self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = x + self.conv6(x)\n",
    "        x = self.conv7(x)\n",
    "        x_mask = self.sigmoid_spatial(x)\n",
    "        res1 = res1 * x_mask\n",
    "        return res2 + res1\n",
    "\n",
    "              \n",
    "class GLSA(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=512, embed_dim=32, k_s=3):\n",
    "        super().__init__()\n",
    "                      \n",
    "        self.conv1_1 = BasicConv2d(embed_dim*2,embed_dim, 1)\n",
    "        self.conv1_1_1 = BasicConv2d(input_dim//2,embed_dim,1)\n",
    "        self.local_11conv = nn.Conv2d(input_dim//2,embed_dim,1)\n",
    "        self.global_11conv = nn.Conv2d(input_dim//2,embed_dim,1)\n",
    "        self.GlobelBlock = ContextBlock(inplanes= embed_dim, ratio=2)\n",
    "        self.local = ConvBranch(in_features = embed_dim, hidden_features = embed_dim, out_features = embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "        x_0, x_1 = x.chunk(2,dim = 1)  \n",
    "        \n",
    "    # local block \n",
    "        local = self.local(self.local_11conv(x_0))\n",
    "        \n",
    "    # Globel block    \n",
    "        Globel = self.GlobelBlock(self.global_11conv(x_1))\n",
    "\n",
    "    # concat Globel + local\n",
    "        x = torch.cat([local,Globel], dim=1)\n",
    "        x = self.conv1_1(x)\n",
    "\n",
    "        return x    \n",
    "\n",
    "class SBA(nn.Module):\n",
    "\n",
    "    def __init__(self,input_dim = 64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.d_in1 = BasicConv2d(input_dim//2, input_dim//2, 1)\n",
    "        self.d_in2 = BasicConv2d(input_dim//2, input_dim//2, 1)       \n",
    "                \n",
    "\n",
    "        self.conv = nn.Sequential(BasicConv2d(input_dim, input_dim, 3,1,1), nn.Conv2d(input_dim, 1, kernel_size=1, bias=False))\n",
    "        self.fc1 = nn.Conv2d(input_dim, input_dim//2, kernel_size=1, bias=False)\n",
    "        self.fc2 = nn.Conv2d(input_dim, input_dim//2, kernel_size=1, bias=False)\n",
    "        \n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, H_feature, L_feature):\n",
    "\n",
    "        L_feature = self.fc1(L_feature)\n",
    "        H_feature = self.fc2(H_feature)\n",
    "        \n",
    "        g_L_feature =  self.Sigmoid(L_feature)\n",
    "        g_H_feature = self.Sigmoid(H_feature)\n",
    "        \n",
    "        L_feature = self.d_in1(L_feature)\n",
    "        H_feature = self.d_in2(H_feature)\n",
    "\n",
    "\n",
    "        L_feature = L_feature + L_feature * g_L_feature + (1 - g_L_feature) * Upsample(g_H_feature * H_feature, size= L_feature.size()[2:], align_corners=False)\n",
    "        H_feature = H_feature + H_feature * g_H_feature + (1 - g_H_feature) * Upsample(g_L_feature * L_feature, size= H_feature.size()[2:], align_corners=False) \n",
    "        \n",
    "        H_feature = Upsample(H_feature, size = L_feature.size()[2:])\n",
    "        out = self.conv(torch.cat([H_feature,L_feature], dim=1))\n",
    "        return out\n",
    "        \n",
    "            \n",
    "class DuAT(nn.Module):\n",
    "    def __init__(self, dim=32, dims= [64, 128, 320, 512], learning_rate=None,  loss_fn=None, optimizer=None, device=None, weight_decay=None):\n",
    "        super(DuAT, self).__init__()\n",
    "\n",
    "                    \n",
    "        if (device is None):\n",
    "            self.device = DEVICE\n",
    "        else:\n",
    "            self.device = device\n",
    "\n",
    "        if (learning_rate is None):\n",
    "            self.learning_rate = 1e-5\n",
    "        else:\n",
    "            self.learning_rate = learning_rate\n",
    "        \n",
    "        if (weight_decay is None):\n",
    "            self.weight_decay = 1e-5\n",
    "        else:\n",
    "            self.weight_decay = weight_decay\n",
    "\n",
    "        self.dims = dims\n",
    "\n",
    "        self.pad = False\n",
    "        self.backbone = pvt_v2_b2()  # [64, 128, 320, 512]\n",
    "        \n",
    "        \n",
    "        c1_in_channels, c2_in_channels, c3_in_channels, c4_in_channels = dims[0], dims[1], dims[2], dims[3]\n",
    "        \n",
    "\n",
    "        self.embed4 = nn.Embedding(48, c4_in_channels)\n",
    "        self.embed3 = nn.Embedding(96, c3_in_channels)\n",
    "        self.embed2 = nn.Embedding(192, c2_in_channels)\n",
    "        self.embed1 = nn.Embedding(384, c1_in_channels)\n",
    "\n",
    "        self.bn4 = nn.BatchNorm2d(c4_in_channels)\n",
    "        self.bn3 = nn.BatchNorm2d(c3_in_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(c2_in_channels)\n",
    "        self.bn1 = nn.BatchNorm2d(c1_in_channels)\n",
    "\n",
    "        self.GLSA_c4 = GLSA(input_dim=c4_in_channels, embed_dim=dim)\n",
    "        self.GLSA_c3 = GLSA(input_dim=c3_in_channels, embed_dim=dim)\n",
    "        self.GLSA_c2 = GLSA(input_dim=c2_in_channels, embed_dim=dim)\n",
    "        self.L_feature = BasicConv2d(c1_in_channels, dim, 3,1,1)\n",
    "        \n",
    "        self.SBA = SBA(input_dim = dim)\n",
    "        self.fuse = BasicConv2d(dim * 2, dim, 1)\n",
    "        self.fuse2 = nn.Sequential(BasicConv2d(dim*3, dim, 1,1),nn.BatchNorm2d(dim),nn.Conv2d(dim, 1, kernel_size=1, bias=False))\n",
    "\n",
    "        \n",
    "\n",
    "        if (loss_fn is None):\n",
    "            self.loss_fn = structure_loss\n",
    "        else:\n",
    "            self.loss_fn = loss_fn\n",
    "\n",
    "        if (optimizer is None):\n",
    "            self.optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        else:\n",
    "            self.optimizer = optimizer(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "\n",
    "        self = self.to(self.device)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # backbone\n",
    "        x = x.to(self.device)\n",
    "        c1, c2, c3, c4 = self.backbone(x)\n",
    "        \n",
    "        n, _, h, w = c4.shape        \n",
    "        _c4 = self.GLSA_c4(c4) # [1, 64, 11, 11]\n",
    "        _c4 = Upsample(_c4, c3.size()[2:])\n",
    "        _c3 = self.GLSA_c3(c3) # [1, 64, 22, 22]\n",
    "        _c2 = self.GLSA_c2(c2) # [1, 64, 44, 44]\n",
    "        \n",
    "        output = torch.cat([Upsample(_c4, c2.size()[2:]), Upsample(_c3, c2.size()[2:]), _c2], dim=1)\n",
    "        \n",
    "        L_feature = self.L_feature(c1)  # [1, 64, 88, 88]\n",
    "        H_feature = self.fuse(torch.cat([_c4, _c3], dim=1))\n",
    "        H_feature = Upsample(H_feature,c2.size()[2:])\n",
    "        \n",
    "        output2 = self.SBA(H_feature,L_feature)\n",
    "        h = x.shape[2] // 4\n",
    "        output = F.interpolate(output, scale_factor=8, mode='bilinear')\n",
    "        # output = torch.cat([output, Upsample(c1, output.size()[2:])], dim=1)\n",
    "        output = self.fuse2(output)\n",
    "        # return torch.sigmoid(output)\n",
    "        output2 = F.interpolate(output2, scale_factor=4, mode='bilinear')\n",
    "        if (self.pad):\n",
    "            output = output[:,:,h:-h,:]\n",
    "            output2 = output2[:,:,h:-h,:]\n",
    "        \n",
    "        return output, output2\n",
    "        # return F.sigmoid(output), F.sigmoid(output2)\n",
    "        \n",
    "    def fit(self, X, mask, y):\n",
    "        X, mask, y = X.to(self.device), mask.to(self.device), y.to(self.device)\n",
    "        h1, h2 = self(X, mask)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.loss_fn(h1, y) + self.loss_fn(h2, y)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        loss = loss.item()\n",
    "        pred = (h2 + h1 > 0).int()\n",
    "        numt = torch.sum(mask)\n",
    "        TP = torch.sum(torch.minimum(y, pred)).item() / numt\n",
    "        TN = torch.sum(torch.minimum(1-y, 1-pred) * mask).item() / numt\n",
    "        FN = torch.sum(torch.minimum(y, 1-pred)).item() / numt\n",
    "        FP = torch.sum(torch.minimum(1-y, pred)).item() / numt\n",
    "        if (torch.any(TP < 0) or torch.any(TN < 0) or torch.any(FN < 0) or torch.any(FP < 0)):\n",
    "            raise AssertionError()\n",
    "        return (loss, torch.tensor([TP, FP, FN, TN]))\n",
    "\n",
    "    def test(self, X, mask, y):\n",
    "        X, mask, y = X.to(self.device), mask.to(self.device), y.to(self.device)\n",
    "        h1, h2 = self(X, mask)\n",
    "        loss = self.loss_fn(h1, y) + self.loss_fn(h2, y)\n",
    "        loss = loss.item()\n",
    "        pred = (h2 + h1 > 0).int()\n",
    "        numt = torch.sum(mask)\n",
    "        TP = torch.sum(torch.minimum(y, pred)).item() / numt\n",
    "        TN = torch.sum(torch.minimum(1-y, 1-pred) * mask).item() / numt\n",
    "        FN = torch.sum(torch.minimum(y, 1-pred)).item() / numt\n",
    "        FP = torch.sum(torch.minimum(1-y, pred)).item() / numt\n",
    "        return (loss, torch.tensor([TP, FP, FN, TN]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ac7a15-568a-4f77-8f4b-c0b4182152f1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Spatial PVT DuAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5466749e-7a38-4105-8d41-c006a99ee413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from lib.pvtv2 import pvt_v2_b2\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from mmcv.cnn import ConvModule\n",
    "from torch.nn import Conv2d, UpsamplingBilinear2d\n",
    "import warnings\n",
    "import torch\n",
    "# from mmcv.cnn import constant_init, kaiming_init\n",
    "from torch import nn\n",
    "from torchvision.transforms.functional import normalize\n",
    "from PVTv2.pvtv2 import pvt_v2_b2\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def structure_loss(pred, mask):\n",
    "    weit = 1 + 5 * torch.abs(F.avg_pool2d(mask, kernel_size=31, stride=1, padding=15) - mask)\n",
    "    wbce = F.binary_cross_entropy_with_logits(pred, mask, reduce='none')\n",
    "    wbce = (weit * wbce).sum(dim=(2, 3)) / weit.sum(dim=(2, 3))\n",
    "\n",
    "    pred = torch.sigmoid(pred)\n",
    "    inter = ((pred * mask) * weit).sum(dim=(2, 3))\n",
    "    union = ((pred + mask) * weit).sum(dim=(2, 3))\n",
    "    wiou = 1 - (inter + 1) / (union - inter + 1)\n",
    "\n",
    "    return (wbce + wiou).mean()\n",
    "\n",
    "# def structure_loss(pred, mask):\n",
    "#     pred = torch.sigmoid(pred)\n",
    "#     loss1 = F.binary_cross_entropy(pred, mask)\n",
    "#     tp = pred * mask\n",
    "#     fp = pred * (1 - mask)\n",
    "#     fn = (1 - pred) * mask\n",
    "#     tp = torch.mean(tp, dim=0)\n",
    "#     fp = torch.mean(fp, dim=0)\n",
    "#     fn = torch.mean(fn, dim=0)\n",
    "#     iou = tp / (tp + fp + fn)\n",
    "#     iou = torch.mean(iou)\n",
    "#     loss2 = 1 - iou\n",
    "#     return loss1 + loss2\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes,\n",
    "                              kernel_size=kernel_size, stride=stride,\n",
    "                              padding=padding, dilation=dilation, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "        \n",
    "class Block(nn.Sequential):\n",
    "    def __init__(self, input_num, num1, num2, dilation_rate, drop_out, bn_start=True, norm_layer=nn.BatchNorm2d):\n",
    "        super(Block, self).__init__()\n",
    "        if bn_start:\n",
    "            self.add_module('norm1', norm_layer(input_num)),\n",
    "\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv1', nn.Conv2d(in_channels=input_num, out_channels=num1, kernel_size=1)),\n",
    "\n",
    "        self.add_module('norm2', norm_layer(num1)),\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv2', nn.Conv2d(in_channels=num1, out_channels=num2, kernel_size=3,\n",
    "                                            dilation=dilation_rate, padding=dilation_rate)),\n",
    "        self.drop_rate = drop_out\n",
    "\n",
    "    def forward(self, _input):\n",
    "        feature = super(Block, self).forward(_input)\n",
    "        if self.drop_rate > 0:\n",
    "            feature = F.dropout2d(feature, p=self.drop_rate, training=self.training)\n",
    "        return feature\n",
    "\n",
    "\n",
    "def Upsample(x, size, align_corners = False):\n",
    "    \"\"\"\n",
    "    Wrapper Around the Upsample Call\n",
    "    \"\"\"\n",
    "    return nn.functional.interpolate(x, size=size, mode='bilinear', align_corners=align_corners)\n",
    "\n",
    "    \n",
    "# def last_zero_init(m):\n",
    "#     if isinstance(m, nn.Sequential):\n",
    "#         constant_init(m[-1], val=0)\n",
    "#     else:\n",
    "#         constant_init(m, val=0)\n",
    "\n",
    "\n",
    "class ContextBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 inplanes,\n",
    "                 ratio,\n",
    "                 pooling_type='att',\n",
    "                 fusion_types=('channel_mul', )):\n",
    "        super(ContextBlock, self).__init__()\n",
    "        assert pooling_type in ['avg', 'att']\n",
    "        assert isinstance(fusion_types, (list, tuple))\n",
    "        valid_fusion_types = ['channel_add', 'channel_mul']\n",
    "        assert all([f in valid_fusion_types for f in fusion_types])\n",
    "        assert len(fusion_types) > 0, 'at least one fusion should be used'\n",
    "        self.inplanes = inplanes\n",
    "        self.ratio = ratio\n",
    "        self.planes = int(inplanes * ratio)\n",
    "        self.pooling_type = pooling_type\n",
    "        self.fusion_types = fusion_types\n",
    "        if pooling_type == 'att':\n",
    "            self.conv_mask = nn.Conv2d(inplanes, 1, kernel_size=1)\n",
    "            self.softmax = nn.Softmax(dim=2)\n",
    "        else:\n",
    "            self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        if 'channel_add' in fusion_types:\n",
    "            self.channel_add_conv = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, self.planes, kernel_size=1),\n",
    "                nn.LayerNorm([self.planes, 1, 1]),\n",
    "                nn.ReLU(inplace=True),  # yapf: disable\n",
    "                nn.Conv2d(self.planes, self.inplanes, kernel_size=1))\n",
    "        else:\n",
    "            self.channel_add_conv = None\n",
    "        if 'channel_mul' in fusion_types:\n",
    "            self.channel_mul_conv = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, self.planes, kernel_size=1),\n",
    "                nn.LayerNorm([self.planes, 1, 1]),\n",
    "                nn.ReLU(inplace=True),  # yapf: disable\n",
    "                nn.Conv2d(self.planes, self.inplanes, kernel_size=1))\n",
    "        else:\n",
    "            self.channel_mul_conv = None\n",
    "\n",
    "    def spatial_pool(self, x):\n",
    "        batch, channel, height, width = x.size()\n",
    "        if self.pooling_type == 'att':\n",
    "            input_x = x\n",
    "            # [N, C, H * W]\n",
    "            input_x = input_x.view(batch, channel, height * width)\n",
    "            # [N, 1, C, H * W]\n",
    "            input_x = input_x.unsqueeze(1)\n",
    "            # [N, 1, H, W]\n",
    "            context_mask = self.conv_mask(x)\n",
    "            # [N, 1, H * W]\n",
    "            context_mask = context_mask.view(batch, 1, height * width)\n",
    "            # [N, 1, H * W]\n",
    "            context_mask = self.softmax(context_mask)\n",
    "            # [N, 1, H * W, 1]\n",
    "            context_mask = context_mask.unsqueeze(-1)\n",
    "            # [N, 1, C, 1]\n",
    "            context = torch.matmul(input_x, context_mask)\n",
    "            # [N, C, 1, 1]\n",
    "            context = context.view(batch, channel, 1, 1)\n",
    "        else:\n",
    "            # [N, C, 1, 1]\n",
    "            context = self.avg_pool(x)\n",
    "\n",
    "        return context\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [N, C, 1, 1]\n",
    "        context = self.spatial_pool(x)\n",
    "\n",
    "        out = x\n",
    "        if self.channel_mul_conv is not None:\n",
    "            # [N, C, 1, 1]\n",
    "            channel_mul_term = torch.sigmoid(self.channel_mul_conv(context))\n",
    "            out = out + out * channel_mul_term\n",
    "        if self.channel_add_conv is not None:\n",
    "            # [N, C, 1, 1]\n",
    "            channel_add_term = self.channel_add_conv(context)\n",
    "            out = out + channel_add_term\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1   = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2   = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "\n",
    "class ConvBranch(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features = None, out_features = None):\n",
    "        super().__init__()\n",
    "        hidden_features = hidden_features or in_features\n",
    "        out_features = out_features or in_features\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_features, hidden_features, 1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, hidden_features, 3, padding=1, groups=hidden_features, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, hidden_features, 1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, hidden_features, 3, padding=1, groups=hidden_features, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, hidden_features, 1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.SiLU(inplace=True)\n",
    "        )\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, hidden_features, 3, padding=1, groups=hidden_features, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv7 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, out_features, 1, bias=False),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.ca = ChannelAttention(64)\n",
    "        self.sa = SpatialAttention()\n",
    "        self.sigmoid_spatial = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        res1 = x\n",
    "        res2 = x\n",
    "        x = self.conv1(x)        \n",
    "        x = x + self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x + self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = x + self.conv6(x)\n",
    "        x = self.conv7(x)\n",
    "        x_mask = self.sigmoid_spatial(x)\n",
    "        res1 = res1 * x_mask\n",
    "        return res2 + res1\n",
    "\n",
    "              \n",
    "class GLSA(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=512, embed_dim=32, k_s=3):\n",
    "        super().__init__()\n",
    "                      \n",
    "        self.conv1_1 = BasicConv2d(embed_dim*2,embed_dim, 1)\n",
    "        self.conv1_1_1 = BasicConv2d(input_dim//2,embed_dim,1)\n",
    "        self.local_11conv = nn.Conv2d(input_dim//2,embed_dim,1)\n",
    "        self.global_11conv = nn.Conv2d(input_dim//2,embed_dim,1)\n",
    "        self.GlobelBlock = ContextBlock(inplanes= embed_dim, ratio=2)\n",
    "        self.local = ConvBranch(in_features = embed_dim, hidden_features = embed_dim, out_features = embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "        x_0, x_1 = x.chunk(2,dim = 1)  \n",
    "        \n",
    "    # local block \n",
    "        local = self.local(self.local_11conv(x_0))\n",
    "        \n",
    "    # Globel block    \n",
    "        Globel = self.GlobelBlock(self.global_11conv(x_1))\n",
    "\n",
    "    # concat Globel + local\n",
    "        x = torch.cat([local,Globel], dim=1)\n",
    "        x = self.conv1_1(x)\n",
    "\n",
    "        return x    \n",
    "\n",
    "class SBA(nn.Module):\n",
    "\n",
    "    def __init__(self,input_dim = 64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.d_in1 = BasicConv2d(input_dim//2, input_dim//2, 1)\n",
    "        self.d_in2 = BasicConv2d(input_dim//2, input_dim//2, 1)       \n",
    "                \n",
    "\n",
    "        self.conv = nn.Sequential(BasicConv2d(input_dim, input_dim, 3,1,1), nn.Conv2d(input_dim, 1, kernel_size=1, bias=False))\n",
    "        self.fc1 = nn.Conv2d(input_dim, input_dim//2, kernel_size=1, bias=False)\n",
    "        self.fc2 = nn.Conv2d(input_dim, input_dim//2, kernel_size=1, bias=False)\n",
    "        \n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, H_feature, L_feature):\n",
    "\n",
    "        L_feature = self.fc1(L_feature)\n",
    "        H_feature = self.fc2(H_feature)\n",
    "        \n",
    "        g_L_feature =  self.Sigmoid(L_feature)\n",
    "        g_H_feature = self.Sigmoid(H_feature)\n",
    "        \n",
    "        L_feature = self.d_in1(L_feature)\n",
    "        H_feature = self.d_in2(H_feature)\n",
    "\n",
    "\n",
    "        L_feature = L_feature + L_feature * g_L_feature + (1 - g_L_feature) * Upsample(g_H_feature * H_feature, size= L_feature.size()[2:], align_corners=False)\n",
    "        H_feature = H_feature + H_feature * g_H_feature + (1 - g_H_feature) * Upsample(g_L_feature * L_feature, size= H_feature.size()[2:], align_corners=False) \n",
    "        \n",
    "        H_feature = Upsample(H_feature, size = L_feature.size()[2:])\n",
    "        out = self.conv(torch.cat([H_feature,L_feature], dim=1))\n",
    "        return out\n",
    "        \n",
    "            \n",
    "class DuAT(nn.Module):\n",
    "    def __init__(self, dim=32, dims= [64, 128, 320, 512], learning_rate=None,  loss_fn=None, optimizer=None, device=None, weight_decay=None):\n",
    "        super(DuAT, self).__init__()\n",
    "\n",
    "                    \n",
    "        if (device is None):\n",
    "            self.device = DEVICE\n",
    "        else:\n",
    "            self.device = device\n",
    "\n",
    "        if (learning_rate is None):\n",
    "            self.learning_rate = 1e-5\n",
    "        else:\n",
    "            self.learning_rate = learning_rate\n",
    "        \n",
    "        if (weight_decay is None):\n",
    "            self.weight_decay = 1e-5\n",
    "        else:\n",
    "            self.weight_decay = weight_decay\n",
    "\n",
    "        self.dims = dims\n",
    "\n",
    "        # self.pad = False\n",
    "        self.backbone = pvt_v2_b2()  # [64, 128, 320, 512]\n",
    "        \n",
    "        c1_in_channels, c2_in_channels, c3_in_channels, c4_in_channels = dims[0], dims[1], dims[2], dims[3]\n",
    "\n",
    "        self.shuffle4 = nn.Conv2d(c4_in_channels, c4_in_channels, 1)\n",
    "        self.shuffle3 = nn.Conv2d(c3_in_channels, c3_in_channels, 1)\n",
    "        self.shuffle2 = nn.Conv2d(c2_in_channels, c2_in_channels, 1)\n",
    "        self.shuffle1 = nn.Conv2d(c1_in_channels, c1_in_channels, 1)\n",
    "\n",
    "        self.bn4 = nn.BatchNorm2d(c4_in_channels)\n",
    "        self.bn3 = nn.BatchNorm2d(c3_in_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(c2_in_channels)\n",
    "        self.bn1 = nn.BatchNorm2d(c1_in_channels)\n",
    "\n",
    "        self.embed4 = nn.Embedding(32, c4_in_channels)\n",
    "        self.embed3 = nn.Embedding(64, c3_in_channels)\n",
    "        self.embed2 = nn.Embedding(128, c2_in_channels)\n",
    "        self.embed1 = nn.Embedding(256, c1_in_channels)\n",
    "\n",
    "        self.bn4 = nn.BatchNorm2d(c4_in_channels)\n",
    "        self.bn3 = nn.BatchNorm2d(c3_in_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(c2_in_channels)\n",
    "        self.bn1 = nn.BatchNorm2d(c1_in_channels)\n",
    "\n",
    "        self.GLSA_c4 = GLSA(input_dim=c4_in_channels, embed_dim=dim)\n",
    "        self.GLSA_c3 = GLSA(input_dim=c3_in_channels, embed_dim=dim)\n",
    "        self.GLSA_c2 = GLSA(input_dim=c2_in_channels, embed_dim=dim)\n",
    "        self.L_feature = BasicConv2d(c1_in_channels, dim, 3,1,1)\n",
    "        \n",
    "        self.SBA = SBA(input_dim = dim)\n",
    "        self.fuse = BasicConv2d(dim * 2, dim, 1)\n",
    "        self.fuse2 = nn.Sequential(BasicConv2d(dim*3, dim, 1,1),nn.BatchNorm2d(dim),nn.Conv2d(dim, 1, kernel_size=1, bias=False))\n",
    "\n",
    "        \n",
    "\n",
    "        if (loss_fn is None):\n",
    "            self.loss_fn = structure_loss\n",
    "        else:\n",
    "            self.loss_fn = loss_fn\n",
    "\n",
    "        if (optimizer is None):\n",
    "            self.optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        else:\n",
    "            self.optimizer = optimizer(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "\n",
    "        self = self.to(self.device)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # backbone\n",
    "        x = x.to(self.device)\n",
    "        c1, c2, c3, c4 = self.backbone(x)\n",
    "\n",
    "        c1_pos = self.embed1(torch.arange(c1.shape[-1], device=self.device))\n",
    "        c1_pos = c1_pos.view(1, self.dims[0], 1, c1.shape[-1]).repeat(c1.shape[0],1,c1.shape[2],1)\n",
    "\n",
    "        c2_pos = self.embed2(torch.arange(c2.shape[-1], device=self.device))\n",
    "        c2_pos = c2_pos.view(1, self.dims[1], 1, c2.shape[-1]).repeat(c2.shape[0],1,c2.shape[2],1)\n",
    "\n",
    "        c3_pos = self.embed3(torch.arange(c3.shape[-1], device=self.device))\n",
    "        c3_pos = c3_pos.view(1, self.dims[2], 1, c3.shape[-1]).repeat(c3.shape[0],1,c3.shape[2],1)\n",
    "\n",
    "        c4_pos = self.embed4(torch.arange(c4.shape[-1], device=self.device))\n",
    "        c4_pos = c4_pos.view(1, self.dims[3], 1, c4.shape[-1]).repeat(c4.shape[0],1,c4.shape[2],1)\n",
    "\n",
    "        \n",
    "        # c1 = self.bn1(self.shuffle1(c1))\n",
    "        # c2 = self.bn2(self.shuffle2(c2))\n",
    "        # c3 = self.bn3(self.shuffle3(c3))\n",
    "        # c4 = self.bn4(self.shuffle4(c4))\n",
    "        \n",
    "        # c1 = torch.cat([c1,\n",
    "        #                 torch.arange(c1.shape[2], dtype=c1.dtype, device=self.device).view(1,1,-1,1).repeat(c1.shape[0], 1, 1, c1.shape[3]) / c1.shape[2],\n",
    "        #                 torch.arange(c1.shape[3], dtype=c1.dtype, device=self.device).view(1,1,1,-1).repeat(c1.shape[0], 1, c1.shape[2], 1) / c1.shape[3]], dim=1)\n",
    "        # c2 = torch.cat([c2,\n",
    "        #                 torch.arange(c2.shape[2], dtype=c2.dtype, device=self.device).view(1,1,-1,1).repeat(c2.shape[0], 1, 1, c2.shape[3]) / c2.shape[2],\n",
    "        #                 torch.arange(c2.shape[3], dtype=c2.dtype, device=self.device).view(1,1,1,-1).repeat(c2.shape[0], 1, c2.shape[2], 1) / c2.shape[3]], dim=1)\n",
    "        # c3 = torch.cat([c3,\n",
    "        #                 torch.arange(c3.shape[2], dtype=c3.dtype, device=self.device).view(1,1,-1,1).repeat(c1.shape[0], 1, 1, c3.shape[3]) / c3.shape[2],\n",
    "        #                 torch.arange(c3.shape[3], dtype=c3.dtype, device=self.device).view(1,1,1,-1).repeat(c1.shape[0], 1, c3.shape[2], 1) / c3.shape[3]], dim=1)\n",
    "        # c4 = torch.cat([c4,\n",
    "        #                 torch.arange(c4.shape[2], dtype=c4.dtype, device=self.device).view(1,1,-1,1).repeat(c4.shape[0], 1, 1, c4.shape[3]) / c4.shape[2],\n",
    "        #                 torch.arange(c4.shape[3], dtype=c4.dtype, device=self.device).view(1,1,1,-1).repeat(c4.shape[0], 1, c4.shape[2], 1) / c4.shape[3]], dim=1)\n",
    "\n",
    "        c1 = self.bn1(c1 + c1_pos)\n",
    "        c2 = self.bn2(c2 + c2_pos)\n",
    "        c3 = self.bn3(c3 + c3_pos)\n",
    "        c4 = self.bn4(c4 + c4_pos)\n",
    "        \n",
    "        n, _, h, w = c4.shape        \n",
    "        _c4 = self.GLSA_c4(c4) # [1, 64, 11, 11]\n",
    "        _c4 = Upsample(_c4, c3.size()[2:])\n",
    "        _c3 = self.GLSA_c3(c3) # [1, 64, 22, 22]\n",
    "        _c2 = self.GLSA_c2(c2) # [1, 64, 44, 44]\n",
    "        \n",
    "        output = torch.cat([Upsample(_c4, c2.size()[2:]), Upsample(_c3, c2.size()[2:]), _c2], dim=1)\n",
    "        \n",
    "        L_feature = self.L_feature(c1)  # [1, 64, 88, 88]\n",
    "        H_feature = self.fuse(torch.cat([_c4, _c3], dim=1))\n",
    "        H_feature = Upsample(H_feature,c2.size()[2:])\n",
    "        \n",
    "        output2 = self.SBA(H_feature,L_feature)\n",
    "        h = x.shape[2] // 4\n",
    "        output = F.interpolate(output, scale_factor=8, mode='bicubic')\n",
    "        # output = torch.cat([output, Upsample(c1, output.size()[2:])], dim=1)\n",
    "        output = self.fuse2(output)\n",
    "        # return torch.sigmoid(output)\n",
    "        output2 = F.interpolate(output2, scale_factor=4, mode='bicubic')\n",
    "\n",
    "        \n",
    "        return output, output2\n",
    "        # return F.sigmoid(output), F.sigmoid(output2)\n",
    "        \n",
    "    def fit(self, X, mask, y):\n",
    "        X, mask, y = X.to(self.device), mask.to(self.device), y.to(self.device)\n",
    "        h1, h2 = self(X, mask)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.loss_fn(h1, y) + self.loss_fn(h2, y)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        loss = loss.item()\n",
    "        pred = (h2 > 0.5).int()\n",
    "        numt = torch.sum(mask)\n",
    "        TP = torch.sum(torch.minimum(y, pred)).item() / numt\n",
    "        TN = torch.sum(torch.minimum(1-y, 1-pred) * mask).item() / numt\n",
    "        FN = torch.sum(torch.minimum(y, 1-pred)).item() / numt\n",
    "        FP = torch.sum(torch.minimum(1-y, pred)).item() / numt\n",
    "        return (loss, torch.tensor([TP, FP, FN, TN]))\n",
    "\n",
    "    def test(self, X, mask, y):\n",
    "        X, mask, y = X.to(self.device), mask.to(self.device), y.to(self.device)\n",
    "        h1, h2 = self(X, mask)\n",
    "        loss = self.loss_fn(h1, y) + self.loss_fn(h2, y)\n",
    "        loss = loss.item()\n",
    "        pred = (h2 > 0.5).int()\n",
    "        numt = torch.sum(mask)\n",
    "        TP = torch.sum(torch.minimum(y, pred)).item() / numt\n",
    "        TN = torch.sum(torch.minimum(1-y, 1-pred) * mask).item() / numt\n",
    "        FN = torch.sum(torch.minimum(y, 1-pred)).item() / numt\n",
    "        FP = torch.sum(torch.minimum(1-y, pred)).item() / numt\n",
    "        return (loss, torch.tensor([TP, FP, FN, TN]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d796df82-4454-42c9-b793-b231f1c65957",
   "metadata": {},
   "source": [
    "# Spatial Flat PVTDuAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0d84cfb-aff3-4382-b539-729142cb6f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from lib.pvtv2 import pvt_v2_b2\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from mmcv.cnn import ConvModule\n",
    "from torch.nn import Conv2d, UpsamplingBilinear2d\n",
    "import warnings\n",
    "import torch\n",
    "# from mmcv.cnn import constant_init, kaiming_init\n",
    "from torch import nn\n",
    "from torchvision.transforms.functional import normalize\n",
    "from BaseModel import BaseModel\n",
    "from PVT.segmentation.pvt import pvt_large, pvt_medium, pvt_small, pvt_tiny\n",
    "from PVTv2.pvtv2 import *\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def structure_loss(pred, mask):\n",
    "    weit = 1 + 5 * torch.abs(F.avg_pool2d(mask, kernel_size=31, stride=1, padding=15) - mask)\n",
    "    wbce = F.binary_cross_entropy_with_logits(pred, mask, reduce='none')\n",
    "    wbce = (weit * wbce).sum(dim=(2, 3)) / weit.sum(dim=(2, 3))\n",
    "\n",
    "    pred = torch.sigmoid(pred)\n",
    "    inter = ((pred * mask) * weit).sum(dim=(2, 3))\n",
    "    union = ((pred + mask) * weit).sum(dim=(2, 3))\n",
    "    wiou = 1 - (inter + 1) / (union - inter + 1)\n",
    "\n",
    "    return (wbce + wiou).mean()\n",
    "\n",
    "# def structure_loss(pred, mask):\n",
    "#     pred = torch.sigmoid(pred)\n",
    "#     loss1 = F.binary_cross_entropy(pred, mask)\n",
    "#     tp = pred * mask\n",
    "#     fp = pred * (1 - mask)\n",
    "#     fn = (1 - pred) * mask\n",
    "#     tp = torch.mean(tp, dim=0)\n",
    "#     fp = torch.mean(fp, dim=0)\n",
    "#     fn = torch.mean(fn, dim=0)\n",
    "#     iou = tp / (tp + fp + fn)\n",
    "#     iou = torch.mean(iou)\n",
    "#     loss2 = 1 - iou\n",
    "#     return loss1 + loss2\n",
    "\n",
    "class SpatBlock(nn.Module):\n",
    "    def __init__(self, dim=32, order=5, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.order = order\n",
    "        self.device = device\n",
    "\n",
    "        self.blocks = nn.ModuleList([nn.Conv2d(1, dim, 1)] + [nn.Conv2d(1, dim, 1, bias=False) for _ in range(order-1)])\n",
    "\n",
    "        self = self.to(self.device)\n",
    "\n",
    "    def forward(self, pos, outi, outj):\n",
    "        posj, posi = pos\n",
    "        posi = posi.to(self.device)\n",
    "        posj = posj.to(self.device)\n",
    "        n = posi.size()[0]\n",
    "\n",
    "        posi = torch.reshape(posi, (n, 1, 1, 1)).to(self.device)\n",
    "        posj = torch.reshape(posj, (n, 1, 1, 1)).to(self.device)\n",
    "\n",
    "        iloc = torch.reshape(torch.linspace(0, 1, outi), (1, 1, 1, outi)).to(self.device) - posi\n",
    "        jloc = torch.reshape(torch.linspace(0, 1, outj), (1, 1, outj, 1)).to(self.device) - posj\n",
    "\n",
    "        raw_pos = (torch.sqrt(iloc**2 + jloc**2)).float()\n",
    "        \n",
    "        \n",
    "        out = torch.zeros(n, self.dim, outi, outj).to(self.device)\n",
    "\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            out += block(raw_pos**(i+1))\n",
    "\n",
    "        return out\n",
    "        \n",
    "        \n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes,\n",
    "                              kernel_size=kernel_size, stride=stride,\n",
    "                              padding=padding, dilation=dilation, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "        \n",
    "class Block(nn.Sequential):\n",
    "    def __init__(self, input_num, num1, num2, dilation_rate, drop_out, bn_start=True, norm_layer=nn.BatchNorm2d):\n",
    "        super(Block, self).__init__()\n",
    "        if bn_start:\n",
    "            self.add_module('norm1', norm_layer(input_num)),\n",
    "\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv1', nn.Conv2d(in_channels=input_num, out_channels=num1, kernel_size=1)),\n",
    "\n",
    "        self.add_module('norm2', norm_layer(num1)),\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv2', nn.Conv2d(in_channels=num1, out_channels=num2, kernel_size=3,\n",
    "                                            dilation=dilation_rate, padding=dilation_rate)),\n",
    "        self.drop_rate = drop_out\n",
    "\n",
    "    def forward(self, _input):\n",
    "        feature = super(Block, self).forward(_input)\n",
    "        if self.drop_rate > 0:\n",
    "            feature = F.dropout2d(feature, p=self.drop_rate, training=self.training)\n",
    "        return feature\n",
    "\n",
    "\n",
    "def Upsample(x, size, align_corners = False):\n",
    "    \"\"\"\n",
    "    Wrapper Around the Upsample Call\n",
    "    \"\"\"\n",
    "    return nn.functional.interpolate(x, size=size, mode='bilinear', align_corners=align_corners)\n",
    "\n",
    "    \n",
    "# def last_zero_init(m):\n",
    "#     if isinstance(m, nn.Sequential):\n",
    "#         constant_init(m[-1], val=0)\n",
    "#     else:\n",
    "#         constant_init(m, val=0)\n",
    "\n",
    "\n",
    "class ContextBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 inplanes,\n",
    "                 ratio,\n",
    "                 pooling_type='att',\n",
    "                 fusion_types=('channel_mul', )):\n",
    "        super(ContextBlock, self).__init__()\n",
    "        assert pooling_type in ['avg', 'att']\n",
    "        assert isinstance(fusion_types, (list, tuple))\n",
    "        valid_fusion_types = ['channel_add', 'channel_mul']\n",
    "        assert all([f in valid_fusion_types for f in fusion_types])\n",
    "        assert len(fusion_types) > 0, 'at least one fusion should be used'\n",
    "        self.inplanes = inplanes\n",
    "        self.ratio = ratio\n",
    "        self.planes = int(inplanes * ratio)\n",
    "        self.pooling_type = pooling_type\n",
    "        self.fusion_types = fusion_types\n",
    "        if pooling_type == 'att':\n",
    "            self.conv_mask = nn.Conv2d(inplanes, 1, kernel_size=1)\n",
    "            self.softmax = nn.Softmax(dim=2)\n",
    "        else:\n",
    "            self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        if 'channel_add' in fusion_types:\n",
    "            self.channel_add_conv = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, self.planes, kernel_size=1),\n",
    "                nn.LayerNorm([self.planes, 1, 1]),\n",
    "                nn.ReLU(inplace=True),  # yapf: disable\n",
    "                nn.Conv2d(self.planes, self.inplanes, kernel_size=1))\n",
    "        else:\n",
    "            self.channel_add_conv = None\n",
    "        if 'channel_mul' in fusion_types:\n",
    "            self.channel_mul_conv = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, self.planes, kernel_size=1),\n",
    "                nn.LayerNorm([self.planes, 1, 1]),\n",
    "                nn.ReLU(inplace=True),  # yapf: disable\n",
    "                nn.Conv2d(self.planes, self.inplanes, kernel_size=1))\n",
    "        else:\n",
    "            self.channel_mul_conv = None\n",
    "\n",
    "    def spatial_pool(self, x):\n",
    "        batch, channel, height, width = x.size()\n",
    "        if self.pooling_type == 'att':\n",
    "            input_x = x\n",
    "            # [N, C, H * W]\n",
    "            input_x = input_x.view(batch, channel, height * width)\n",
    "            # [N, 1, C, H * W]\n",
    "            input_x = input_x.unsqueeze(1)\n",
    "            # [N, 1, H, W]\n",
    "            context_mask = self.conv_mask(x)\n",
    "            # [N, 1, H * W]\n",
    "            context_mask = context_mask.view(batch, 1, height * width)\n",
    "            # [N, 1, H * W]\n",
    "            context_mask = self.softmax(context_mask)\n",
    "            # [N, 1, H * W, 1]\n",
    "            context_mask = context_mask.unsqueeze(-1)\n",
    "            # [N, 1, C, 1]\n",
    "            context = torch.matmul(input_x, context_mask)\n",
    "            # [N, C, 1, 1]\n",
    "            context = context.view(batch, channel, 1, 1)\n",
    "        else:\n",
    "            # [N, C, 1, 1]\n",
    "            context = self.avg_pool(x)\n",
    "\n",
    "        return context\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [N, C, 1, 1]\n",
    "        context = self.spatial_pool(x)\n",
    "\n",
    "        out = x\n",
    "        if self.channel_mul_conv is not None:\n",
    "            # [N, C, 1, 1]\n",
    "            channel_mul_term = torch.sigmoid(self.channel_mul_conv(context))\n",
    "            out = out + out * channel_mul_term\n",
    "        if self.channel_add_conv is not None:\n",
    "            # [N, C, 1, 1]\n",
    "            channel_add_term = self.channel_add_conv(context)\n",
    "            out = out + channel_add_term\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1   = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2   = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "\n",
    "class ConvBranch(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features = None, out_features = None):\n",
    "        super().__init__()\n",
    "        hidden_features = hidden_features or in_features\n",
    "        out_features = out_features or in_features\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_features, hidden_features, 1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, hidden_features, 3, padding=1, groups=hidden_features, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, hidden_features, 1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, hidden_features, 3, padding=1, groups=hidden_features, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, hidden_features, 1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.SiLU(inplace=True)\n",
    "        )\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, hidden_features, 3, padding=1, groups=hidden_features, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv7 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, out_features, 1, bias=False),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.ca = ChannelAttention(64)\n",
    "        self.sa = SpatialAttention()\n",
    "        self.sigmoid_spatial = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        res1 = x\n",
    "        res2 = x\n",
    "        x = self.conv1(x)        \n",
    "        x = x + self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x + self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = x + self.conv6(x)\n",
    "        x = self.conv7(x)\n",
    "        x_mask = self.sigmoid_spatial(x)\n",
    "        res1 = res1 * x_mask\n",
    "        return res2 + res1\n",
    "\n",
    "              \n",
    "class GLSA(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=512, embed_dim=32, k_s=3):\n",
    "        super().__init__()\n",
    "                      \n",
    "        self.conv1_1 = BasicConv2d(embed_dim*2,embed_dim, 1)\n",
    "        self.conv1_1_1 = BasicConv2d(input_dim//2,embed_dim,1)\n",
    "        self.local_11conv = nn.Conv2d(input_dim//2,embed_dim,1)\n",
    "        self.global_11conv = nn.Conv2d(input_dim//2,embed_dim,1)\n",
    "        self.GlobelBlock = ContextBlock(inplanes= embed_dim, ratio=2)\n",
    "        self.local = ConvBranch(in_features = embed_dim, hidden_features = embed_dim, out_features = embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "        x_0, x_1 = x.chunk(2,dim = 1)  \n",
    "        \n",
    "    # local block \n",
    "        local = self.local(self.local_11conv(x_0))\n",
    "        \n",
    "    # Globel block    \n",
    "        Globel = self.GlobelBlock(self.global_11conv(x_1))\n",
    "\n",
    "    # concat Globel + local\n",
    "        x = torch.cat([local,Globel], dim=1)\n",
    "        x = self.conv1_1(x)\n",
    "\n",
    "        return x    \n",
    "\n",
    "class SBA(nn.Module):\n",
    "\n",
    "    def __init__(self,input_dim = 64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.d_in1 = BasicConv2d(input_dim//2, input_dim//2, 1)\n",
    "        self.d_in2 = BasicConv2d(input_dim//2, input_dim//2, 1)       \n",
    "                \n",
    "\n",
    "        self.conv = nn.Sequential(BasicConv2d(input_dim, input_dim, 3,1,1), nn.Conv2d(input_dim, 1, kernel_size=1, bias=False))\n",
    "        self.fc1 = nn.Conv2d(input_dim, input_dim//2, kernel_size=1, bias=False)\n",
    "        self.fc2 = nn.Conv2d(input_dim, input_dim//2, kernel_size=1, bias=False)\n",
    "        \n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, H_feature, L_feature):\n",
    "\n",
    "        L_feature = self.fc1(L_feature)\n",
    "        H_feature = self.fc2(H_feature)\n",
    "        \n",
    "        g_L_feature =  self.Sigmoid(L_feature)\n",
    "        g_H_feature = self.Sigmoid(H_feature)\n",
    "        \n",
    "        L_feature = self.d_in1(L_feature)\n",
    "        H_feature = self.d_in2(H_feature)\n",
    "\n",
    "\n",
    "        L_feature = L_feature + L_feature * g_L_feature + (1 - g_L_feature) * Upsample(g_H_feature * H_feature, size= L_feature.size()[2:], align_corners=False)\n",
    "        H_feature = H_feature + H_feature * g_H_feature + (1 - g_H_feature) * Upsample(g_L_feature * L_feature, size= H_feature.size()[2:], align_corners=False) \n",
    "        \n",
    "        H_feature = Upsample(H_feature, size = L_feature.size()[2:])\n",
    "        out = self.conv(torch.cat([H_feature,L_feature], dim=1))\n",
    "        return out\n",
    "        \n",
    "            \n",
    "class DuAT(nn.Module):\n",
    "    def __init__(self, dim=32, dims= [64, 128, 320, 512], spat_order=3, learning_rate=None,  loss_fn=None, optimizer=None, device=None, weight_decay=None):\n",
    "        super(DuAT, self).__init__()\n",
    "\n",
    "                    \n",
    "        if (device is None):\n",
    "            self.device = DEVICE\n",
    "        else:\n",
    "            self.device = device\n",
    "\n",
    "        if (learning_rate is None):\n",
    "            self.learning_rate = 1e-5\n",
    "        else:\n",
    "            self.learning_rate = learning_rate\n",
    "        \n",
    "        if (weight_decay is None):\n",
    "            self.weight_decay = 1e-5\n",
    "        else:\n",
    "            self.weight_decay = weight_decay\n",
    "\n",
    "        self.dims = dims\n",
    "\n",
    "        self.pad = False\n",
    "        self.backbone = pvt_v2_b2()  # [64, 128, 320, 512]\n",
    "        \n",
    "        \n",
    "        c1_in_channels, c2_in_channels, c3_in_channels, c4_in_channels = dims[0], dims[1], dims[2], dims[3]\n",
    "        \n",
    "\n",
    "        self.spat4 = SpatBlock(dim=dims[3], order=spat_order, device=self.device)\n",
    "        self.spat3 = SpatBlock(dim=dims[2], order=spat_order, device=self.device)\n",
    "        self.spat2 = SpatBlock(dim=dims[1], order=spat_order, device=self.device)\n",
    "        self.spat1 = SpatBlock(dim=dims[0], order=spat_order, device=self.device)\n",
    "\n",
    "        self.bn4 = nn.BatchNorm2d(c4_in_channels)\n",
    "        self.bn3 = nn.BatchNorm2d(c3_in_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(c2_in_channels)\n",
    "        self.bn1 = nn.BatchNorm2d(c1_in_channels)\n",
    "\n",
    "        self.GLSA_c4 = GLSA(input_dim=c4_in_channels, embed_dim=dim)\n",
    "        self.GLSA_c3 = GLSA(input_dim=c3_in_channels, embed_dim=dim)\n",
    "        self.GLSA_c2 = GLSA(input_dim=c2_in_channels, embed_dim=dim)\n",
    "        self.L_feature = BasicConv2d(c1_in_channels, dim, 3,1,1)\n",
    "        \n",
    "        self.SBA = SBA(input_dim = dim)\n",
    "        self.fuse = BasicConv2d(dim * 2, dim, 1)\n",
    "        self.fuse2 = nn.Sequential(BasicConv2d(dim*3, dim, 1,1),nn.BatchNorm2d(dim),nn.Conv2d(dim, 1, kernel_size=1, bias=False))\n",
    "\n",
    "        \n",
    "\n",
    "        if (loss_fn is None):\n",
    "            self.loss_fn = structure_loss\n",
    "        else:\n",
    "            self.loss_fn = loss_fn\n",
    "\n",
    "        if (optimizer is None):\n",
    "            self.optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        else:\n",
    "            self.optimizer = optimizer(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "\n",
    "        self = self.to(self.device)\n",
    "\n",
    "    def forward(self, x, pos):\n",
    "        # backbone\n",
    "        x = x.to(self.device)\n",
    "        c1, c2, c3, c4 = self.backbone(x)\n",
    "\n",
    "        c1 = c1 + self.spat1(pos, c1.shape[2], c1.shape[2])\n",
    "        c2 = c2 + self.spat2(pos, c2.shape[2], c2.shape[2])\n",
    "        c3 = c3 + self.spat3(pos, c3.shape[2], c3.shape[2])\n",
    "        c4 = c4 + self.spat4(pos, c4.shape[2], c4.shape[2])\n",
    "\n",
    "        c1 = self.bn1(c1)\n",
    "        c2 = self.bn2(c2)\n",
    "        c3 = self.bn3(c3)\n",
    "        c4 = self.bn4(c4)\n",
    "        \n",
    "        n, _, h, w = c4.shape        \n",
    "        _c4 = self.GLSA_c4(c4) # [1, 64, 11, 11]\n",
    "        _c4 = Upsample(_c4, c3.size()[2:])\n",
    "        _c3 = self.GLSA_c3(c3) # [1, 64, 22, 22]\n",
    "        _c2 = self.GLSA_c2(c2) # [1, 64, 44, 44]\n",
    "        \n",
    "        output = torch.cat([Upsample(_c4, c2.size()[2:]), Upsample(_c3, c2.size()[2:]), _c2], dim=1)\n",
    "        \n",
    "        L_feature = self.L_feature(c1)  # [1, 64, 88, 88]\n",
    "        H_feature = self.fuse(torch.cat([_c4, _c3], dim=1))\n",
    "        H_feature = Upsample(H_feature,c2.size()[2:])\n",
    "        \n",
    "        output2 = self.SBA(H_feature,L_feature)\n",
    "        h = x.shape[2] // 4\n",
    "        output = F.interpolate(output, scale_factor=8, mode='bilinear')\n",
    "        # output = torch.cat([output, Upsample(c1, output.size()[2:])], dim=1)\n",
    "        output = self.fuse2(output)\n",
    "        # return torch.sigmoid(output)\n",
    "        output2 = F.interpolate(output2, scale_factor=4, mode='bilinear')\n",
    "        if (self.pad):\n",
    "            output = output[:,:,h:-h,:]\n",
    "            output2 = output2[:,:,h:-h,:]\n",
    "        \n",
    "        return output, output2\n",
    "        # return F.sigmoid(output[:,:,h:-h,:])\n",
    "        \n",
    "    def fit(self, X, pos, y):\n",
    "        X, y = X.to(self.device), y.to(self.device)\n",
    "        h1, h2 = self(X, pos)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.loss_fn(h1, y) + self.loss_fn(h2, y)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        loss = loss.item()\n",
    "        pred = (h2 > 0).int()\n",
    "        numt = torch.numel(y)\n",
    "        TP = torch.sum(torch.minimum(y, pred)).item() / numt\n",
    "        TN = torch.sum(torch.minimum(1-y, 1-pred)).item() / numt\n",
    "        FN = torch.sum(torch.minimum(y, 1-pred)).item() / numt\n",
    "        FP = torch.sum(torch.minimum(1-y, pred)).item() / numt\n",
    "        return (loss, torch.tensor([TP, FP, FN, TN]))\n",
    "\n",
    "    def test(self, X, pos, y):\n",
    "        X, y = X.to(self.device), y.to(self.device)\n",
    "        h1, h2 = self(X, pos)\n",
    "        loss = self.loss_fn(h1, y) + self.loss_fn(h2, y)\n",
    "        loss = loss.item()\n",
    "        pred = (h2 > 0).int()\n",
    "        numt = torch.numel(y)\n",
    "        TP = torch.sum(torch.minimum(y, pred)).item() / numt\n",
    "        TN = torch.sum(torch.minimum(1-y, 1-pred)).item() / numt\n",
    "        FN = torch.sum(torch.minimum(y, 1-pred)).item() / numt\n",
    "        FP = torch.sum(torch.minimum(1-y, pred)).item() / numt\n",
    "        return (loss, torch.tensor([TP, FP, FN, TN]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5455e26-9547-42c6-adea-4bdc9e8bb855",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Polynomial Spatial DuAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95a4ffb5-8ab5-4508-afa9-028a44648e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from lib.pvtv2 import pvt_v2_b2\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from mmcv.cnn import ConvModule\n",
    "from torch.nn import Conv2d, UpsamplingBilinear2d\n",
    "import warnings\n",
    "import torch\n",
    "# from mmcv.cnn import constant_init, kaiming_init\n",
    "from torch import nn\n",
    "from torchvision.transforms.functional import normalize\n",
    "from PVTv2.pvtv2 import pvt_v2_b2\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SpatBlock(nn.Module):\n",
    "    def __init__(self, dim=32, order=5):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.order = order\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        self.blocks = nn.ModuleList([nn.Conv2d(1, dim, 1)] + [nn.Conv2d(1, dim, 1, bias=False) for _ in range(order-1)])\n",
    "\n",
    "        self = self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        pos = torch.arange(W) / W\n",
    "        pos = torch.reshape(pos, (1, 1, 1, W)).to(self.device)\n",
    "        \n",
    "        func = torch.zeros_like(x)\n",
    "\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            func += block(pos**(i+1))\n",
    "\n",
    "        out = x + func\n",
    "        \n",
    "        return out\n",
    "        \n",
    "\n",
    "def structure_loss(pred, mask):\n",
    "    weit = 1 + 5 * torch.abs(F.avg_pool2d(mask, kernel_size=31, stride=1, padding=15) - mask)\n",
    "    wbce = F.binary_cross_entropy_with_logits(pred, mask, reduce='none')\n",
    "    wbce = (weit * wbce).sum(dim=(2, 3)) / weit.sum(dim=(2, 3))\n",
    "\n",
    "    pred = torch.sigmoid(pred)\n",
    "    inter = ((pred * mask) * weit).sum(dim=(2, 3))\n",
    "    union = ((pred + mask) * weit).sum(dim=(2, 3))\n",
    "    wiou = 1 - (inter + 1) / (union - inter + 1)\n",
    "\n",
    "    return (wbce + wiou).mean()\n",
    "\n",
    "# def structure_loss(pred, mask):\n",
    "#     pred = torch.sigmoid(pred)\n",
    "#     loss1 = F.binary_cross_entropy(pred, mask)\n",
    "#     tp = pred * mask\n",
    "#     fp = pred * (1 - mask)\n",
    "#     fn = (1 - pred) * mask\n",
    "#     tp = torch.mean(tp, dim=0)\n",
    "#     fp = torch.mean(fp, dim=0)\n",
    "#     fn = torch.mean(fn, dim=0)\n",
    "#     iou = tp / (tp + fp + fn)\n",
    "#     iou = torch.mean(iou)\n",
    "#     loss2 = 1 - iou\n",
    "#     return loss1 + loss2\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes,\n",
    "                              kernel_size=kernel_size, stride=stride,\n",
    "                              padding=padding, dilation=dilation, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "        \n",
    "class Block(nn.Sequential):\n",
    "    def __init__(self, input_num, num1, num2, dilation_rate, drop_out, bn_start=True, norm_layer=nn.BatchNorm2d):\n",
    "        super(Block, self).__init__()\n",
    "        if bn_start:\n",
    "            self.add_module('norm1', norm_layer(input_num)),\n",
    "\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv1', nn.Conv2d(in_channels=input_num, out_channels=num1, kernel_size=1)),\n",
    "\n",
    "        self.add_module('norm2', norm_layer(num1)),\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv2', nn.Conv2d(in_channels=num1, out_channels=num2, kernel_size=3,\n",
    "                                            dilation=dilation_rate, padding=dilation_rate)),\n",
    "        self.drop_rate = drop_out\n",
    "\n",
    "    def forward(self, _input):\n",
    "        feature = super(Block, self).forward(_input)\n",
    "        if self.drop_rate > 0:\n",
    "            feature = F.dropout2d(feature, p=self.drop_rate, training=self.training)\n",
    "        return feature\n",
    "\n",
    "\n",
    "def Upsample(x, size, align_corners = False):\n",
    "    \"\"\"\n",
    "    Wrapper Around the Upsample Call\n",
    "    \"\"\"\n",
    "    return nn.functional.interpolate(x, size=size, mode='bilinear', align_corners=align_corners)\n",
    "\n",
    "    \n",
    "# def last_zero_init(m):\n",
    "#     if isinstance(m, nn.Sequential):\n",
    "#         constant_init(m[-1], val=0)\n",
    "#     else:\n",
    "#         constant_init(m, val=0)\n",
    "\n",
    "\n",
    "class ContextBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 inplanes,\n",
    "                 ratio,\n",
    "                 pooling_type='att',\n",
    "                 fusion_types=('channel_mul', )):\n",
    "        super(ContextBlock, self).__init__()\n",
    "        assert pooling_type in ['avg', 'att']\n",
    "        assert isinstance(fusion_types, (list, tuple))\n",
    "        valid_fusion_types = ['channel_add', 'channel_mul']\n",
    "        assert all([f in valid_fusion_types for f in fusion_types])\n",
    "        assert len(fusion_types) > 0, 'at least one fusion should be used'\n",
    "        self.inplanes = inplanes\n",
    "        self.ratio = ratio\n",
    "        self.planes = int(inplanes * ratio)\n",
    "        self.pooling_type = pooling_type\n",
    "        self.fusion_types = fusion_types\n",
    "        if pooling_type == 'att':\n",
    "            self.conv_mask = nn.Conv2d(inplanes, 1, kernel_size=1)\n",
    "            self.softmax = nn.Softmax(dim=2)\n",
    "        else:\n",
    "            self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        if 'channel_add' in fusion_types:\n",
    "            self.channel_add_conv = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, self.planes, kernel_size=1),\n",
    "                nn.LayerNorm([self.planes, 1, 1]),\n",
    "                nn.ReLU(inplace=True),  # yapf: disable\n",
    "                nn.Conv2d(self.planes, self.inplanes, kernel_size=1))\n",
    "        else:\n",
    "            self.channel_add_conv = None\n",
    "        if 'channel_mul' in fusion_types:\n",
    "            self.channel_mul_conv = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, self.planes, kernel_size=1),\n",
    "                nn.LayerNorm([self.planes, 1, 1]),\n",
    "                nn.ReLU(inplace=True),  # yapf: disable\n",
    "                nn.Conv2d(self.planes, self.inplanes, kernel_size=1))\n",
    "        else:\n",
    "            self.channel_mul_conv = None\n",
    "\n",
    "    def spatial_pool(self, x):\n",
    "        batch, channel, height, width = x.size()\n",
    "        if self.pooling_type == 'att':\n",
    "            input_x = x\n",
    "            # [N, C, H * W]\n",
    "            input_x = input_x.view(batch, channel, height * width)\n",
    "            # [N, 1, C, H * W]\n",
    "            input_x = input_x.unsqueeze(1)\n",
    "            # [N, 1, H, W]\n",
    "            context_mask = self.conv_mask(x)\n",
    "            # [N, 1, H * W]\n",
    "            context_mask = context_mask.view(batch, 1, height * width)\n",
    "            # [N, 1, H * W]\n",
    "            context_mask = self.softmax(context_mask)\n",
    "            # [N, 1, H * W, 1]\n",
    "            context_mask = context_mask.unsqueeze(-1)\n",
    "            # [N, 1, C, 1]\n",
    "            context = torch.matmul(input_x, context_mask)\n",
    "            # [N, C, 1, 1]\n",
    "            context = context.view(batch, channel, 1, 1)\n",
    "        else:\n",
    "            # [N, C, 1, 1]\n",
    "            context = self.avg_pool(x)\n",
    "\n",
    "        return context\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [N, C, 1, 1]\n",
    "        context = self.spatial_pool(x)\n",
    "\n",
    "        out = x\n",
    "        if self.channel_mul_conv is not None:\n",
    "            # [N, C, 1, 1]\n",
    "            channel_mul_term = torch.sigmoid(self.channel_mul_conv(context))\n",
    "            out = out + out * channel_mul_term\n",
    "        if self.channel_add_conv is not None:\n",
    "            # [N, C, 1, 1]\n",
    "            channel_add_term = self.channel_add_conv(context)\n",
    "            out = out + channel_add_term\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1   = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2   = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "\n",
    "class ConvBranch(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features = None, out_features = None):\n",
    "        super().__init__()\n",
    "        hidden_features = hidden_features or in_features\n",
    "        out_features = out_features or in_features\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_features, hidden_features, 1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, hidden_features, 3, padding=1, groups=hidden_features, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, hidden_features, 1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, hidden_features, 3, padding=1, groups=hidden_features, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, hidden_features, 1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.SiLU(inplace=True)\n",
    "        )\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, hidden_features, 3, padding=1, groups=hidden_features, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv7 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, out_features, 1, bias=False),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.ca = ChannelAttention(64)\n",
    "        self.sa = SpatialAttention()\n",
    "        self.sigmoid_spatial = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        res1 = x\n",
    "        res2 = x\n",
    "        x = self.conv1(x)        \n",
    "        x = x + self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x + self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = x + self.conv6(x)\n",
    "        x = self.conv7(x)\n",
    "        x_mask = self.sigmoid_spatial(x)\n",
    "        res1 = res1 * x_mask\n",
    "        return res2 + res1\n",
    "\n",
    "              \n",
    "class GLSA(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=512, embed_dim=32, k_s=3):\n",
    "        super().__init__()\n",
    "                      \n",
    "        self.conv1_1 = BasicConv2d(embed_dim*2,embed_dim, 1)\n",
    "        self.conv1_1_1 = BasicConv2d(input_dim//2,embed_dim,1)\n",
    "        self.local_11conv = nn.Conv2d(input_dim//2,embed_dim,1)\n",
    "        self.global_11conv = nn.Conv2d(input_dim//2,embed_dim,1)\n",
    "        self.GlobelBlock = ContextBlock(inplanes= embed_dim, ratio=2)\n",
    "        self.local = ConvBranch(in_features = embed_dim, hidden_features = embed_dim, out_features = embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "        x_0, x_1 = x.chunk(2,dim = 1)  \n",
    "        \n",
    "    # local block \n",
    "        local = self.local(self.local_11conv(x_0))\n",
    "        \n",
    "    # Globel block    \n",
    "        Globel = self.GlobelBlock(self.global_11conv(x_1))\n",
    "\n",
    "    # concat Globel + local\n",
    "        x = torch.cat([local,Globel], dim=1)\n",
    "        x = self.conv1_1(x)\n",
    "\n",
    "        return x    \n",
    "\n",
    "class SBA(nn.Module):\n",
    "\n",
    "    def __init__(self,input_dim = 64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.d_in1 = BasicConv2d(input_dim//2, input_dim//2, 1)\n",
    "        self.d_in2 = BasicConv2d(input_dim//2, input_dim//2, 1)       \n",
    "                \n",
    "\n",
    "        self.conv = nn.Sequential(BasicConv2d(input_dim, input_dim, 3,1,1), nn.Conv2d(input_dim, 1, kernel_size=1, bias=False))\n",
    "        self.fc1 = nn.Conv2d(input_dim, input_dim//2, kernel_size=1, bias=False)\n",
    "        self.fc2 = nn.Conv2d(input_dim, input_dim//2, kernel_size=1, bias=False)\n",
    "        \n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, H_feature, L_feature):\n",
    "\n",
    "        L_feature = self.fc1(L_feature)\n",
    "        H_feature = self.fc2(H_feature)\n",
    "        \n",
    "        g_L_feature =  self.Sigmoid(L_feature)\n",
    "        g_H_feature = self.Sigmoid(H_feature)\n",
    "        \n",
    "        L_feature = self.d_in1(L_feature)\n",
    "        H_feature = self.d_in2(H_feature)\n",
    "\n",
    "\n",
    "        L_feature = L_feature + L_feature * g_L_feature + (1 - g_L_feature) * Upsample(g_H_feature * H_feature, size= L_feature.size()[2:], align_corners=False)\n",
    "        H_feature = H_feature + H_feature * g_H_feature + (1 - g_H_feature) * Upsample(g_L_feature * L_feature, size= H_feature.size()[2:], align_corners=False) \n",
    "        \n",
    "        H_feature = Upsample(H_feature, size = L_feature.size()[2:])\n",
    "        out = self.conv(torch.cat([H_feature,L_feature], dim=1))\n",
    "        return out\n",
    "        \n",
    "            \n",
    "class DuAT(nn.Module):\n",
    "    def __init__(self, dim=32, dims= [64, 128, 320, 512], learning_rate=None,  loss_fn=None, optimizer=None, device=None, weight_decay=None):\n",
    "        super(DuAT, self).__init__()\n",
    "\n",
    "                    \n",
    "        if (device is None):\n",
    "            self.device = DEVICE\n",
    "        else:\n",
    "            self.device = device\n",
    "\n",
    "        if (learning_rate is None):\n",
    "            self.learning_rate = 1e-5\n",
    "        else:\n",
    "            self.learning_rate = learning_rate\n",
    "        \n",
    "        if (weight_decay is None):\n",
    "            self.weight_decay = 1e-5\n",
    "        else:\n",
    "            self.weight_decay = weight_decay\n",
    "\n",
    "        self.dims = dims\n",
    "\n",
    "        # self.pad = False\n",
    "        self.backbone = pvt_v2_b2()  # [64, 128, 320, 512]\n",
    "        \n",
    "        c1_in_channels, c2_in_channels, c3_in_channels, c4_in_channels = dims[0], dims[1], dims[2], dims[3]\n",
    "\n",
    "        self.shuffle4 = nn.Conv2d(c4_in_channels, c4_in_channels, 1)\n",
    "        self.shuffle3 = nn.Conv2d(c3_in_channels, c3_in_channels, 1)\n",
    "        self.shuffle2 = nn.Conv2d(c2_in_channels, c2_in_channels, 1)\n",
    "        self.shuffle1 = nn.Conv2d(c1_in_channels, c1_in_channels, 1)\n",
    "\n",
    "        self.bn4 = nn.BatchNorm2d(c4_in_channels)\n",
    "        self.bn3 = nn.BatchNorm2d(c3_in_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(c2_in_channels)\n",
    "        self.bn1 = nn.BatchNorm2d(c1_in_channels)\n",
    "\n",
    "        self.embed4 = SpatBlock(dim=c4_in_channels)\n",
    "        self.embed3 = SpatBlock(dim=c3_in_channels)\n",
    "        self.embed2 = SpatBlock(dim=c2_in_channels)\n",
    "        self.embed1 = SpatBlock(dim=c1_in_channels)\n",
    "\n",
    "        self.bn4 = nn.BatchNorm2d(c4_in_channels)\n",
    "        self.bn3 = nn.BatchNorm2d(c3_in_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(c2_in_channels)\n",
    "        self.bn1 = nn.BatchNorm2d(c1_in_channels)\n",
    "\n",
    "        self.GLSA_c4 = GLSA(input_dim=c4_in_channels, embed_dim=dim)\n",
    "        self.GLSA_c3 = GLSA(input_dim=c3_in_channels, embed_dim=dim)\n",
    "        self.GLSA_c2 = GLSA(input_dim=c2_in_channels, embed_dim=dim)\n",
    "        self.L_feature = BasicConv2d(c1_in_channels, dim, 3,1,1)\n",
    "        \n",
    "        self.SBA = SBA(input_dim = dim)\n",
    "        self.fuse = BasicConv2d(dim * 2, dim, 1)\n",
    "        self.fuse2 = nn.Sequential(BasicConv2d(dim*3, dim, 1,1),nn.BatchNorm2d(dim),nn.Conv2d(dim, 1, kernel_size=1, bias=False))\n",
    "\n",
    "        \n",
    "\n",
    "        if (loss_fn is None):\n",
    "            self.loss_fn = structure_loss\n",
    "        else:\n",
    "            self.loss_fn = loss_fn\n",
    "\n",
    "        if (optimizer is None):\n",
    "            self.optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        else:\n",
    "            self.optimizer = optimizer(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "\n",
    "        self = self.to(self.device)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # backbone\n",
    "        x = x.to(self.device)\n",
    "        c1, c2, c3, c4 = self.backbone(x)\n",
    "\n",
    "\n",
    "        \n",
    "        # c1 = self.bn1(self.shuffle1(c1))\n",
    "        # c2 = self.bn2(self.shuffle2(c2))\n",
    "        # c3 = self.bn3(self.shuffle3(c3))\n",
    "        # c4 = self.bn4(self.shuffle4(c4))\n",
    "        \n",
    "        # c1 = torch.cat([c1,\n",
    "        #                 torch.arange(c1.shape[2], dtype=c1.dtype, device=self.device).view(1,1,-1,1).repeat(c1.shape[0], 1, 1, c1.shape[3]) / c1.shape[2],\n",
    "        #                 torch.arange(c1.shape[3], dtype=c1.dtype, device=self.device).view(1,1,1,-1).repeat(c1.shape[0], 1, c1.shape[2], 1) / c1.shape[3]], dim=1)\n",
    "        # c2 = torch.cat([c2,\n",
    "        #                 torch.arange(c2.shape[2], dtype=c2.dtype, device=self.device).view(1,1,-1,1).repeat(c2.shape[0], 1, 1, c2.shape[3]) / c2.shape[2],\n",
    "        #                 torch.arange(c2.shape[3], dtype=c2.dtype, device=self.device).view(1,1,1,-1).repeat(c2.shape[0], 1, c2.shape[2], 1) / c2.shape[3]], dim=1)\n",
    "        # c3 = torch.cat([c3,\n",
    "        #                 torch.arange(c3.shape[2], dtype=c3.dtype, device=self.device).view(1,1,-1,1).repeat(c1.shape[0], 1, 1, c3.shape[3]) / c3.shape[2],\n",
    "        #                 torch.arange(c3.shape[3], dtype=c3.dtype, device=self.device).view(1,1,1,-1).repeat(c1.shape[0], 1, c3.shape[2], 1) / c3.shape[3]], dim=1)\n",
    "        # c4 = torch.cat([c4,\n",
    "        #                 torch.arange(c4.shape[2], dtype=c4.dtype, device=self.device).view(1,1,-1,1).repeat(c4.shape[0], 1, 1, c4.shape[3]) / c4.shape[2],\n",
    "        #                 torch.arange(c4.shape[3], dtype=c4.dtype, device=self.device).view(1,1,1,-1).repeat(c4.shape[0], 1, c4.shape[2], 1) / c4.shape[3]], dim=1)\n",
    "\n",
    "        c1 = self.bn1(self.embed1(c1))\n",
    "        c2 = self.bn2(self.embed2(c2))\n",
    "        c3 = self.bn3(self.embed3(c3))\n",
    "        c4 = self.bn4(self.embed4(c4))\n",
    "        \n",
    "        n, _, h, w = c4.shape        \n",
    "        _c4 = self.GLSA_c4(c4) # [1, 64, 11, 11]\n",
    "        _c4 = Upsample(_c4, c3.size()[2:])\n",
    "        _c3 = self.GLSA_c3(c3) # [1, 64, 22, 22]\n",
    "        _c2 = self.GLSA_c2(c2) # [1, 64, 44, 44]\n",
    "        \n",
    "        output = torch.cat([Upsample(_c4, c2.size()[2:]), Upsample(_c3, c2.size()[2:]), _c2], dim=1)\n",
    "        \n",
    "        L_feature = self.L_feature(c1)  # [1, 64, 88, 88]\n",
    "        H_feature = self.fuse(torch.cat([_c4, _c3], dim=1))\n",
    "        H_feature = Upsample(H_feature,c2.size()[2:])\n",
    "        \n",
    "        output2 = self.SBA(H_feature,L_feature)\n",
    "        h = x.shape[2] // 4\n",
    "        output = F.interpolate(output, scale_factor=8, mode='bicubic')\n",
    "        # output = torch.cat([output, Upsample(c1, output.size()[2:])], dim=1)\n",
    "        output = self.fuse2(output)\n",
    "        # return torch.sigmoid(output)\n",
    "        output2 = F.interpolate(output2, scale_factor=4, mode='bicubic')\n",
    "\n",
    "        \n",
    "        return output, output2\n",
    "        # return F.sigmoid(output[:,:,h:-h,:])\n",
    "        \n",
    "    def fit(self, X, mask, y):\n",
    "        X, mask, y = X.to(self.device), mask.to(self.device), y.to(self.device)\n",
    "        h1, h2 = self(X, mask)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.loss_fn(h1, y) + self.loss_fn(h2, y)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        loss = loss.item()\n",
    "        pred = (h2 > 0).int()\n",
    "        numt = torch.sum(mask)\n",
    "        TP = torch.sum(torch.minimum(y, pred)).item() / numt\n",
    "        TN = torch.sum(torch.minimum(1-y, 1-pred) * mask).item() / numt\n",
    "        FN = torch.sum(torch.minimum(y, 1-pred)).item() / numt\n",
    "        FP = torch.sum(torch.minimum(1-y, pred)).item() / numt\n",
    "        return (loss, torch.tensor([TP, FP, FN, TN]))\n",
    "\n",
    "    def test(self, X, mask, y):\n",
    "        X, mask, y = X.to(self.device), mask.to(self.device), y.to(self.device)\n",
    "        h1, h2 = self(X, mask)\n",
    "        loss = self.loss_fn(h1, y) + self.loss_fn(h2, y)\n",
    "        loss = loss.item()\n",
    "        pred = (h2 > 0).int()\n",
    "        numt = torch.sum(mask)\n",
    "        TP = torch.sum(torch.minimum(y, pred)).item() / numt\n",
    "        TN = torch.sum(torch.minimum(1-y, 1-pred) * mask).item() / numt\n",
    "        FN = torch.sum(torch.minimum(y, 1-pred)).item() / numt\n",
    "        FP = torch.sum(torch.minimum(1-y, pred)).item() / numt\n",
    "        return (loss, torch.tensor([TP, FP, FN, TN]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d26903e-09ab-4814-b0b9-127d08730e3d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Tanh Polynomial Spatial PVT DuAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c592329d-5e48-4161-b7e9-d0456d298b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from lib.pvtv2 import pvt_v2_b2\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from mmcv.cnn import ConvModule\n",
    "from torch.nn import Conv2d, UpsamplingBilinear2d\n",
    "import warnings\n",
    "import torch\n",
    "# from mmcv.cnn import constant_init, kaiming_init\n",
    "from torch import nn\n",
    "from torchvision.transforms.functional import normalize\n",
    "from PVTv2.pvtv2 import pvt_v2_b2\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class SpatBlock(nn.Module):\n",
    "    def __init__(self, dim=32, order=5):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.order = order\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        self.blocks = nn.ModuleList([nn.Conv2d(1, dim, 1)] + [nn.Conv2d(1, dim, 1, bias=False) for _ in range(order-1)])\n",
    "\n",
    "        self = self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        pos = torch.arange(W) / W\n",
    "        pos = torch.reshape(pos, (1, 1, 1, W)).to(self.device)\n",
    "        \n",
    "        func = torch.zeros_like(x)\n",
    "\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            func += block(pos**(i+1))\n",
    "\n",
    "        out = F.tanh(func) * x\n",
    "        \n",
    "        return out\n",
    "        \n",
    "\n",
    "def structure_loss(pred, mask):\n",
    "    weit = 1 + 5 * torch.abs(F.avg_pool2d(mask, kernel_size=31, stride=1, padding=15) - mask)\n",
    "    wbce = F.binary_cross_entropy_with_logits(pred, mask, reduce='none')\n",
    "    wbce = (weit * wbce).sum(dim=(2, 3)) / weit.sum(dim=(2, 3))\n",
    "\n",
    "    pred = torch.sigmoid(pred)\n",
    "    inter = ((pred * mask) * weit).sum(dim=(2, 3))\n",
    "    union = ((pred + mask) * weit).sum(dim=(2, 3))\n",
    "    wiou = 1 - (inter + 1) / (union - inter + 1)\n",
    "\n",
    "    return (wbce + wiou).mean()\n",
    "\n",
    "# def structure_loss(pred, mask):\n",
    "#     pred = torch.sigmoid(pred)\n",
    "#     loss1 = F.binary_cross_entropy(pred, mask)\n",
    "#     tp = pred * mask\n",
    "#     fp = pred * (1 - mask)\n",
    "#     fn = (1 - pred) * mask\n",
    "#     tp = torch.mean(tp, dim=0)\n",
    "#     fp = torch.mean(fp, dim=0)\n",
    "#     fn = torch.mean(fn, dim=0)\n",
    "#     iou = tp / (tp + fp + fn)\n",
    "#     iou = torch.mean(iou)\n",
    "#     loss2 = 1 - iou\n",
    "#     return loss1 + loss2\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes,\n",
    "                              kernel_size=kernel_size, stride=stride,\n",
    "                              padding=padding, dilation=dilation, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "        \n",
    "class Block(nn.Sequential):\n",
    "    def __init__(self, input_num, num1, num2, dilation_rate, drop_out, bn_start=True, norm_layer=nn.BatchNorm2d):\n",
    "        super(Block, self).__init__()\n",
    "        if bn_start:\n",
    "            self.add_module('norm1', norm_layer(input_num)),\n",
    "\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv1', nn.Conv2d(in_channels=input_num, out_channels=num1, kernel_size=1)),\n",
    "\n",
    "        self.add_module('norm2', norm_layer(num1)),\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv2', nn.Conv2d(in_channels=num1, out_channels=num2, kernel_size=3,\n",
    "                                            dilation=dilation_rate, padding=dilation_rate)),\n",
    "        self.drop_rate = drop_out\n",
    "\n",
    "    def forward(self, _input):\n",
    "        feature = super(Block, self).forward(_input)\n",
    "        if self.drop_rate > 0:\n",
    "            feature = F.dropout2d(feature, p=self.drop_rate, training=self.training)\n",
    "        return feature\n",
    "\n",
    "\n",
    "def Upsample(x, size, align_corners = False):\n",
    "    \"\"\"\n",
    "    Wrapper Around the Upsample Call\n",
    "    \"\"\"\n",
    "    return nn.functional.interpolate(x, size=size, mode='bilinear', align_corners=align_corners)\n",
    "\n",
    "    \n",
    "# def last_zero_init(m):\n",
    "#     if isinstance(m, nn.Sequential):\n",
    "#         constant_init(m[-1], val=0)\n",
    "#     else:\n",
    "#         constant_init(m, val=0)\n",
    "\n",
    "\n",
    "class ContextBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 inplanes,\n",
    "                 ratio,\n",
    "                 pooling_type='att',\n",
    "                 fusion_types=('channel_mul', )):\n",
    "        super(ContextBlock, self).__init__()\n",
    "        assert pooling_type in ['avg', 'att']\n",
    "        assert isinstance(fusion_types, (list, tuple))\n",
    "        valid_fusion_types = ['channel_add', 'channel_mul']\n",
    "        assert all([f in valid_fusion_types for f in fusion_types])\n",
    "        assert len(fusion_types) > 0, 'at least one fusion should be used'\n",
    "        self.inplanes = inplanes\n",
    "        self.ratio = ratio\n",
    "        self.planes = int(inplanes * ratio)\n",
    "        self.pooling_type = pooling_type\n",
    "        self.fusion_types = fusion_types\n",
    "        if pooling_type == 'att':\n",
    "            self.conv_mask = nn.Conv2d(inplanes, 1, kernel_size=1)\n",
    "            self.softmax = nn.Softmax(dim=2)\n",
    "        else:\n",
    "            self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        if 'channel_add' in fusion_types:\n",
    "            self.channel_add_conv = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, self.planes, kernel_size=1),\n",
    "                nn.LayerNorm([self.planes, 1, 1]),\n",
    "                nn.ReLU(inplace=True),  # yapf: disable\n",
    "                nn.Conv2d(self.planes, self.inplanes, kernel_size=1))\n",
    "        else:\n",
    "            self.channel_add_conv = None\n",
    "        if 'channel_mul' in fusion_types:\n",
    "            self.channel_mul_conv = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, self.planes, kernel_size=1),\n",
    "                nn.LayerNorm([self.planes, 1, 1]),\n",
    "                nn.ReLU(inplace=True),  # yapf: disable\n",
    "                nn.Conv2d(self.planes, self.inplanes, kernel_size=1))\n",
    "        else:\n",
    "            self.channel_mul_conv = None\n",
    "\n",
    "    def spatial_pool(self, x):\n",
    "        batch, channel, height, width = x.size()\n",
    "        if self.pooling_type == 'att':\n",
    "            input_x = x\n",
    "            # [N, C, H * W]\n",
    "            input_x = input_x.view(batch, channel, height * width)\n",
    "            # [N, 1, C, H * W]\n",
    "            input_x = input_x.unsqueeze(1)\n",
    "            # [N, 1, H, W]\n",
    "            context_mask = self.conv_mask(x)\n",
    "            # [N, 1, H * W]\n",
    "            context_mask = context_mask.view(batch, 1, height * width)\n",
    "            # [N, 1, H * W]\n",
    "            context_mask = self.softmax(context_mask)\n",
    "            # [N, 1, H * W, 1]\n",
    "            context_mask = context_mask.unsqueeze(-1)\n",
    "            # [N, 1, C, 1]\n",
    "            context = torch.matmul(input_x, context_mask)\n",
    "            # [N, C, 1, 1]\n",
    "            context = context.view(batch, channel, 1, 1)\n",
    "        else:\n",
    "            # [N, C, 1, 1]\n",
    "            context = self.avg_pool(x)\n",
    "\n",
    "        return context\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [N, C, 1, 1]\n",
    "        context = self.spatial_pool(x)\n",
    "\n",
    "        out = x\n",
    "        if self.channel_mul_conv is not None:\n",
    "            # [N, C, 1, 1]\n",
    "            channel_mul_term = torch.sigmoid(self.channel_mul_conv(context))\n",
    "            out = out + out * channel_mul_term\n",
    "        if self.channel_add_conv is not None:\n",
    "            # [N, C, 1, 1]\n",
    "            channel_add_term = self.channel_add_conv(context)\n",
    "            out = out + channel_add_term\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc1   = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2   = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "\n",
    "class ConvBranch(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features = None, out_features = None):\n",
    "        super().__init__()\n",
    "        hidden_features = hidden_features or in_features\n",
    "        out_features = out_features or in_features\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_features, hidden_features, 1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, hidden_features, 3, padding=1, groups=hidden_features, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, hidden_features, 1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, hidden_features, 3, padding=1, groups=hidden_features, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, hidden_features, 1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.SiLU(inplace=True)\n",
    "        )\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, hidden_features, 3, padding=1, groups=hidden_features, bias=False),\n",
    "            nn.BatchNorm2d(hidden_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv7 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_features, out_features, 1, bias=False),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.ca = ChannelAttention(64)\n",
    "        self.sa = SpatialAttention()\n",
    "        self.sigmoid_spatial = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        res1 = x\n",
    "        res2 = x\n",
    "        x = self.conv1(x)        \n",
    "        x = x + self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x + self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = x + self.conv6(x)\n",
    "        x = self.conv7(x)\n",
    "        x_mask = self.sigmoid_spatial(x)\n",
    "        res1 = res1 * x_mask\n",
    "        return res2 + res1\n",
    "\n",
    "              \n",
    "class GLSA(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=512, embed_dim=32, k_s=3):\n",
    "        super().__init__()\n",
    "                      \n",
    "        self.conv1_1 = BasicConv2d(embed_dim*2,embed_dim, 1)\n",
    "        self.conv1_1_1 = BasicConv2d(input_dim//2,embed_dim,1)\n",
    "        self.local_11conv = nn.Conv2d(input_dim//2,embed_dim,1)\n",
    "        self.global_11conv = nn.Conv2d(input_dim//2,embed_dim,1)\n",
    "        self.GlobelBlock = ContextBlock(inplanes= embed_dim, ratio=2)\n",
    "        self.local = ConvBranch(in_features = embed_dim, hidden_features = embed_dim, out_features = embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "        x_0, x_1 = x.chunk(2,dim = 1)  \n",
    "        \n",
    "    # local block \n",
    "        local = self.local(self.local_11conv(x_0))\n",
    "        \n",
    "    # Globel block    \n",
    "        Globel = self.GlobelBlock(self.global_11conv(x_1))\n",
    "\n",
    "    # concat Globel + local\n",
    "        x = torch.cat([local,Globel], dim=1)\n",
    "        x = self.conv1_1(x)\n",
    "\n",
    "        return x    \n",
    "\n",
    "class SBA(nn.Module):\n",
    "\n",
    "    def __init__(self,input_dim = 64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.d_in1 = BasicConv2d(input_dim//2, input_dim//2, 1)\n",
    "        self.d_in2 = BasicConv2d(input_dim//2, input_dim//2, 1)       \n",
    "                \n",
    "\n",
    "        self.conv = nn.Sequential(BasicConv2d(input_dim, input_dim, 3,1,1), nn.Conv2d(input_dim, 1, kernel_size=1, bias=False))\n",
    "        self.fc1 = nn.Conv2d(input_dim, input_dim//2, kernel_size=1, bias=False)\n",
    "        self.fc2 = nn.Conv2d(input_dim, input_dim//2, kernel_size=1, bias=False)\n",
    "        \n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, H_feature, L_feature):\n",
    "\n",
    "        L_feature = self.fc1(L_feature)\n",
    "        H_feature = self.fc2(H_feature)\n",
    "        \n",
    "        g_L_feature =  self.Sigmoid(L_feature)\n",
    "        g_H_feature = self.Sigmoid(H_feature)\n",
    "        \n",
    "        L_feature = self.d_in1(L_feature)\n",
    "        H_feature = self.d_in2(H_feature)\n",
    "\n",
    "\n",
    "        L_feature = L_feature + L_feature * g_L_feature + (1 - g_L_feature) * Upsample(g_H_feature * H_feature, size= L_feature.size()[2:], align_corners=False)\n",
    "        H_feature = H_feature + H_feature * g_H_feature + (1 - g_H_feature) * Upsample(g_L_feature * L_feature, size= H_feature.size()[2:], align_corners=False) \n",
    "        \n",
    "        H_feature = Upsample(H_feature, size = L_feature.size()[2:])\n",
    "        out = self.conv(torch.cat([H_feature,L_feature], dim=1))\n",
    "        return out\n",
    "        \n",
    "            \n",
    "class DuAT(nn.Module):\n",
    "    def __init__(self, dim=32, dims= [64, 128, 320, 512], learning_rate=None,  loss_fn=None, optimizer=None, device=None, weight_decay=None):\n",
    "        super(DuAT, self).__init__()\n",
    "\n",
    "                    \n",
    "        if (device is None):\n",
    "            self.device = DEVICE\n",
    "        else:\n",
    "            self.device = device\n",
    "\n",
    "        if (learning_rate is None):\n",
    "            self.learning_rate = 1e-5\n",
    "        else:\n",
    "            self.learning_rate = learning_rate\n",
    "        \n",
    "        if (weight_decay is None):\n",
    "            self.weight_decay = 1e-5\n",
    "        else:\n",
    "            self.weight_decay = weight_decay\n",
    "\n",
    "        self.dims = dims\n",
    "\n",
    "        # self.pad = False\n",
    "        self.backbone = pvt_v2_b2()  # [64, 128, 320, 512]\n",
    "        \n",
    "        c1_in_channels, c2_in_channels, c3_in_channels, c4_in_channels = dims[0], dims[1], dims[2], dims[3]\n",
    "\n",
    "        self.shuffle4 = nn.Conv2d(c4_in_channels, c4_in_channels, 1)\n",
    "        self.shuffle3 = nn.Conv2d(c3_in_channels, c3_in_channels, 1)\n",
    "        self.shuffle2 = nn.Conv2d(c2_in_channels, c2_in_channels, 1)\n",
    "        self.shuffle1 = nn.Conv2d(c1_in_channels, c1_in_channels, 1)\n",
    "\n",
    "        self.bn4 = nn.BatchNorm2d(c4_in_channels)\n",
    "        self.bn3 = nn.BatchNorm2d(c3_in_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(c2_in_channels)\n",
    "        self.bn1 = nn.BatchNorm2d(c1_in_channels)\n",
    "\n",
    "        self.embed4 = SpatBlock(dim=c4_in_channels)\n",
    "        self.embed3 = SpatBlock(dim=c3_in_channels)\n",
    "        self.embed2 = SpatBlock(dim=c2_in_channels)\n",
    "        self.embed1 = SpatBlock(dim=c1_in_channels)\n",
    "\n",
    "        self.bn4 = nn.BatchNorm2d(c4_in_channels)\n",
    "        self.bn3 = nn.BatchNorm2d(c3_in_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(c2_in_channels)\n",
    "        self.bn1 = nn.BatchNorm2d(c1_in_channels)\n",
    "\n",
    "        self.GLSA_c4 = GLSA(input_dim=c4_in_channels, embed_dim=dim)\n",
    "        self.GLSA_c3 = GLSA(input_dim=c3_in_channels, embed_dim=dim)\n",
    "        self.GLSA_c2 = GLSA(input_dim=c2_in_channels, embed_dim=dim)\n",
    "        self.L_feature = BasicConv2d(c1_in_channels, dim, 3,1,1)\n",
    "        \n",
    "        self.SBA = SBA(input_dim = dim)\n",
    "        self.fuse = BasicConv2d(dim * 2, dim, 1)\n",
    "        self.fuse2 = nn.Sequential(BasicConv2d(dim*3, dim, 1,1),nn.BatchNorm2d(dim),nn.Conv2d(dim, 1, kernel_size=1, bias=False))\n",
    "\n",
    "        \n",
    "\n",
    "        if (loss_fn is None):\n",
    "            self.loss_fn = structure_loss\n",
    "        else:\n",
    "            self.loss_fn = loss_fn\n",
    "\n",
    "        if (optimizer is None):\n",
    "            self.optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        else:\n",
    "            self.optimizer = optimizer(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "\n",
    "        self = self.to(self.device)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # backbone\n",
    "        x = x.to(self.device)\n",
    "        c1, c2, c3, c4 = self.backbone(x)\n",
    "\n",
    "\n",
    "        \n",
    "        # c1 = self.bn1(self.shuffle1(c1))\n",
    "        # c2 = self.bn2(self.shuffle2(c2))\n",
    "        # c3 = self.bn3(self.shuffle3(c3))\n",
    "        # c4 = self.bn4(self.shuffle4(c4))\n",
    "        \n",
    "        # c1 = torch.cat([c1,\n",
    "        #                 torch.arange(c1.shape[2], dtype=c1.dtype, device=self.device).view(1,1,-1,1).repeat(c1.shape[0], 1, 1, c1.shape[3]) / c1.shape[2],\n",
    "        #                 torch.arange(c1.shape[3], dtype=c1.dtype, device=self.device).view(1,1,1,-1).repeat(c1.shape[0], 1, c1.shape[2], 1) / c1.shape[3]], dim=1)\n",
    "        # c2 = torch.cat([c2,\n",
    "        #                 torch.arange(c2.shape[2], dtype=c2.dtype, device=self.device).view(1,1,-1,1).repeat(c2.shape[0], 1, 1, c2.shape[3]) / c2.shape[2],\n",
    "        #                 torch.arange(c2.shape[3], dtype=c2.dtype, device=self.device).view(1,1,1,-1).repeat(c2.shape[0], 1, c2.shape[2], 1) / c2.shape[3]], dim=1)\n",
    "        # c3 = torch.cat([c3,\n",
    "        #                 torch.arange(c3.shape[2], dtype=c3.dtype, device=self.device).view(1,1,-1,1).repeat(c1.shape[0], 1, 1, c3.shape[3]) / c3.shape[2],\n",
    "        #                 torch.arange(c3.shape[3], dtype=c3.dtype, device=self.device).view(1,1,1,-1).repeat(c1.shape[0], 1, c3.shape[2], 1) / c3.shape[3]], dim=1)\n",
    "        # c4 = torch.cat([c4,\n",
    "        #                 torch.arange(c4.shape[2], dtype=c4.dtype, device=self.device).view(1,1,-1,1).repeat(c4.shape[0], 1, 1, c4.shape[3]) / c4.shape[2],\n",
    "        #                 torch.arange(c4.shape[3], dtype=c4.dtype, device=self.device).view(1,1,1,-1).repeat(c4.shape[0], 1, c4.shape[2], 1) / c4.shape[3]], dim=1)\n",
    "\n",
    "        c1 = self.bn1(self.embed1(c1))\n",
    "        c2 = self.bn2(self.embed2(c2))\n",
    "        c3 = self.bn3(self.embed3(c3))\n",
    "        c4 = self.bn4(self.embed4(c4))\n",
    "        \n",
    "        n, _, h, w = c4.shape        \n",
    "        _c4 = self.GLSA_c4(c4) # [1, 64, 11, 11]\n",
    "        _c4 = Upsample(_c4, c3.size()[2:])\n",
    "        _c3 = self.GLSA_c3(c3) # [1, 64, 22, 22]\n",
    "        _c2 = self.GLSA_c2(c2) # [1, 64, 44, 44]\n",
    "        \n",
    "        output = torch.cat([Upsample(_c4, c2.size()[2:]), Upsample(_c3, c2.size()[2:]), _c2], dim=1)\n",
    "        \n",
    "        L_feature = self.L_feature(c1)  # [1, 64, 88, 88]\n",
    "        H_feature = self.fuse(torch.cat([_c4, _c3], dim=1))\n",
    "        H_feature = Upsample(H_feature,c2.size()[2:])\n",
    "        \n",
    "        output2 = self.SBA(H_feature,L_feature)\n",
    "        h = x.shape[2] // 4\n",
    "        output = F.interpolate(output, scale_factor=8, mode='bicubic')\n",
    "        # output = torch.cat([output, Upsample(c1, output.size()[2:])], dim=1)\n",
    "        output = self.fuse2(output)\n",
    "        # return torch.sigmoid(output)\n",
    "        output2 = F.interpolate(output2, scale_factor=4, mode='bicubic')\n",
    "\n",
    "        \n",
    "        return output, output2\n",
    "        # return F.sigmoid(output[:,:,h:-h,:])\n",
    "        \n",
    "    def fit(self, X, mask, y):\n",
    "        X, mask, y = X.to(self.device), mask.to(self.device), y.to(self.device)\n",
    "        h1, h2 = self(X, mask)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.loss_fn(h1, y) + self.loss_fn(h2, y)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        loss = loss.item()\n",
    "        pred = (h2 > 0).int()\n",
    "        numt = torch.sum(mask)\n",
    "        TP = torch.sum(torch.minimum(y, pred)).item() / numt\n",
    "        TN = torch.sum(torch.minimum(1-y, 1-pred) * mask).item() / numt\n",
    "        FN = torch.sum(torch.minimum(y, 1-pred)).item() / numt\n",
    "        FP = torch.sum(torch.minimum(1-y, pred)).item() / numt\n",
    "        return (loss, torch.tensor([TP, FP, FN, TN]))\n",
    "\n",
    "    def test(self, X, mask, y):\n",
    "        X, mask, y = X.to(self.device), mask.to(self.device), y.to(self.device)\n",
    "        h1, h2 = self(X, mask)\n",
    "        loss = self.loss_fn(h1, y) + self.loss_fn(h2, y)\n",
    "        loss = loss.item()\n",
    "        pred = (h2 > 0).int()\n",
    "        numt = torch.sum(mask)\n",
    "        TP = torch.sum(torch.minimum(y, pred)).item() / numt\n",
    "        TN = torch.sum(torch.minimum(1-y, 1-pred) * mask).item() / numt\n",
    "        FN = torch.sum(torch.minimum(y, 1-pred)).item() / numt\n",
    "        FP = torch.sum(torch.minimum(1-y, pred)).item() / numt\n",
    "        return (loss, torch.tensor([TP, FP, FN, TN]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76f92c2-5131-42a1-b1fd-861a33c41589",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Polar test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edcd6f73-4f3f-424e-b448-6373f468b428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tensor_full(tensor, center):\n",
    "    tensor_np = tensor\n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    # closing = tensor_np\n",
    "    closing = cv.morphologyEx(tensor_np, cv.MORPH_ERODE, kernel)\n",
    "    _, labels = cv.connectedComponents(closing)\n",
    "    mask = np.isin(labels, labels[:, 0])\n",
    "    cleaned = np.where(mask, closing, 0)\n",
    "    # cleaned = closing\n",
    "    cleaned = cv.morphologyEx(cleaned, cv.MORPH_DILATE, kernel)\n",
    "\n",
    "    \n",
    "    \n",
    "    value = np.sqrt(((cleaned.shape[1]/2.0)**2.0)+((cleaned.shape[0]/2.0)**2.0))\n",
    "    \n",
    "    polar_image = cv.linearPolar(cleaned, center, value, cv.WARP_FILL_OUTLIERS + cv.WARP_INVERSE_MAP)\n",
    "    # polar_image = closing\n",
    "    kernel = np.ones((10, 10),np.uint8)\n",
    "\n",
    "    cleaned = cv.morphologyEx(polar_image, cv.MORPH_CLOSE, kernel)\n",
    "    return cleaned\n",
    "\n",
    "def process_tensor_pol(tensor):\n",
    "    tensor_np = tensor\n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    # closing = tensor_np\n",
    "    closing = cv.morphologyEx(tensor_np, cv.MORPH_ERODE, kernel)\n",
    "    _, labels = cv.connectedComponents(closing)\n",
    "    mask = np.isin(labels, labels[:, 0])\n",
    "    cleaned = np.where(mask, closing, 0)\n",
    "    # cleaned = closing\n",
    "    cleaned = cv.morphologyEx(cleaned, cv.MORPH_DILATE, kernel)\n",
    "    \n",
    "    kernel = np.ones((10, 10),np.uint8)\n",
    "\n",
    "    cleaned = cv.morphologyEx(cleaned, cv.MORPH_CLOSE, kernel)\n",
    "    return cleaned\n",
    "\n",
    "def process_tensor_basic(tensor, center):\n",
    "    value = np.sqrt(((tensor.shape[1]/2.0)**2.0)+((tensor.shape[0]/2.0)**2.0))\n",
    "    \n",
    "    polar_image = cv.linearPolar(tensor, center, value, cv.WARP_FILL_OUTLIERS + cv.WARP_INVERSE_MAP)\n",
    "    return polar_image\n",
    "\n",
    "process_tensor = process_tensor_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e5ea23b-77ea-446d-811a-baf8de9e74c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "def output(model):\n",
    "    num_batches = len(test_dl)\n",
    "    model.eval()\n",
    "    with torch.no_grad():                \n",
    "        num_batches = len(test_dl)\n",
    "        dices = []\n",
    "        with tqdm(total=num_batches) as pbar:\n",
    "            pbar.set_description(\"Validation Dataset\")\n",
    "            for (X, mask_l, mask_s, y, filename, center) in test_dl:\n",
    "                X = X.to(DEVICE)\n",
    "                mask_s = mask_s.to(DEVICE)\n",
    "                h1, h2 = model(X, mask_s)\n",
    "                h = h1 + h2\n",
    "                h = h.detach().cpu().numpy()[0,0]\n",
    "                h = cv.resize(h, (512, 384))\n",
    "                h = np.uint8(h > .0)\n",
    "                h = process_tensor_full(h, (int(center[0].item() * 512), int(center[1].item() * 384)))\n",
    "                y_tilde = np.round(h)\n",
    "                y = to_cart(y.detach().cpu().numpy()[0,0], (int(center[0].item() * 512), int(center[1].item() * 384)))\n",
    "                y = np.round(y)\n",
    "                # print(type(y), type(y_tilde))\n",
    "                tp = np.sum(np.minimum(y_tilde, y))\n",
    "                fp = np.sum(np.minimum(y_tilde, 1 - y))\n",
    "                fn = np.sum(np.minimum(1 - y_tilde, y))\n",
    "                tn = np.sum(np.minimum(1 - y_tilde, 1 - y))\n",
    "                dice = 2 * tp / (2 * tp + fp + fn)\n",
    "                dices.append(dice)\n",
    "                # img = np.concatenate([y_tilde * 255, y * 255], axis=1)\n",
    "                # cv.imwrite(os.path.join('Final-Improv', f'{dice:.4f}-' + filename[0] + '.jpg'), img)\n",
    "                pbar.update(1)\n",
    "        print(np.mean(np.array(dices)))\n",
    "        results.append(np.mean(np.array(dices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79f9bdff-6702-40db-bed0-dcc9321728c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pol = []\n",
    "\n",
    "def raw_output(model):\n",
    "    num_batches = len(test_dl)\n",
    "    model.eval()\n",
    "    with torch.no_grad():                \n",
    "        num_batches = len(test_dl)\n",
    "        dices = []\n",
    "        with tqdm(total=num_batches) as pbar:\n",
    "            pbar.set_description(\"Validation Dataset\")\n",
    "            for (X, mask_l, mask_s, y, filename, center) in test_dl:\n",
    "                X = X.to(DEVICE)\n",
    "                mask_s = mask_s.to(DEVICE)\n",
    "                h1, h2 = model(X, mask_s)\n",
    "                h = h1 + h2\n",
    "                h = h.detach().cpu().numpy()[0,0]\n",
    "                h = cv.resize(h, (512, 384))\n",
    "                h = np.uint8(h > .0)\n",
    "                h = process_tensor_pol(h)\n",
    "                y_tilde = np.round(h)\n",
    "                y = y.detach().cpu().numpy()[0,0]\n",
    "                y = np.round(y)\n",
    "                # print(type(y), type(y_tilde))\n",
    "                tp = np.sum(np.minimum(y_tilde, y))\n",
    "                fp = np.sum(np.minimum(y_tilde, 1 - y))\n",
    "                fn = np.sum(np.minimum(1 - y_tilde, y))\n",
    "                tn = np.sum(np.minimum(1 - y_tilde, 1 - y))\n",
    "                dice = 2 * tp / (2 * tp + fp + fn)\n",
    "                dices.append(dice)\n",
    "                # img = np.concatenate([y_tilde * 255, y * 255], axis=1)\n",
    "                # cv.imwrite(os.path.join('Final-Improv', f'{dice:.4f}-' + filename[0] + '.jpg'), img)\n",
    "                pbar.update(1)\n",
    "        print(np.mean(np.array(dices)))\n",
    "        results_pol.append(np.mean(np.array(dices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9d1e2d-9c3a-4397-9418-c468fa089fca",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afd73e5c-01cb-422b-a5ef-6d9e5381631b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 ===========\n",
      "Epoch 1 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.9436, Avg. Accuracy: 0.8434, Avg. Dice: 0.8696: 100%|| 519/519 [15:04<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.5190, Avg. Accuracy: 0.9059, Avg. Dice: 0.9230: 100%|| 65/65 [01:06<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9230, Loss:  1.5190\n",
      "Epoch 2 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.5416, Avg. Accuracy: 0.8971, Avg. Dice: 0.9146: 100%|| 519/519 [14:42<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.3397, Avg. Accuracy: 0.9066, Avg. Dice: 0.9230: 100%|| 65/65 [01:07<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9230, Loss:  1.3397\n",
      "Epoch 3 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.3403, Avg. Accuracy: 0.9074, Avg. Dice: 0.9238: 100%|| 519/519 [14:45<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.1502, Avg. Accuracy: 0.9296, Avg. Dice: 0.9420: 100%|| 65/65 [01:07<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9420, Loss:  1.1502\n",
      "Epoch 4 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.2353, Avg. Accuracy: 0.9145, Avg. Dice: 0.9292: 100%|| 519/519 [14:46<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.1262, Avg. Accuracy: 0.9166, Avg. Dice: 0.9308: 100%|| 65/65 [01:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9308, Loss:  1.1262\n",
      "Epoch 5 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.1750, Avg. Accuracy: 0.9195, Avg. Dice: 0.9340: 100%|| 519/519 [14:44<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0586, Avg. Accuracy: 0.9316, Avg. Dice: 0.9444: 100%|| 65/65 [01:07<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9444, Loss:  1.0586\n",
      "Epoch 6 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.1321, Avg. Accuracy: 0.9214, Avg. Dice: 0.9344: 100%|| 519/519 [14:48<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0768, Avg. Accuracy: 0.9263, Avg. Dice: 0.9383: 100%|| 65/65 [01:08<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9383, Loss:  1.0768\n",
      "Epoch 7 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.1006, Avg. Accuracy: 0.9249, Avg. Dice: 0.9378: 100%|| 519/519 [14:43<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0617, Avg. Accuracy: 0.9207, Avg. Dice: 0.9344: 100%|| 65/65 [01:09<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9344, Loss:  1.0617\n",
      "Epoch 8 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0901, Avg. Accuracy: 0.9264, Avg. Dice: 0.9395: 100%|| 519/519 [14:44<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0067, Avg. Accuracy: 0.9295, Avg. Dice: 0.9406: 100%|| 65/65 [01:07<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9406, Loss:  1.0067\n",
      "Epoch 9 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0691, Avg. Accuracy: 0.9269, Avg. Dice: 0.9398: 100%|| 519/519 [14:43<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9912, Avg. Accuracy: 0.9296, Avg. Dice: 0.9419: 100%|| 65/65 [01:08<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9419, Loss:  0.9912\n",
      "Epoch 10 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0388, Avg. Accuracy: 0.9305, Avg. Dice: 0.9424: 100%|| 519/519 [14:41<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9996, Avg. Accuracy: 0.9345, Avg. Dice: 0.9465: 100%|| 65/65 [01:07<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9465, Loss:  0.9996\n",
      "Epoch 11 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0346, Avg. Accuracy: 0.9314, Avg. Dice: 0.9437: 100%|| 519/519 [14:44<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0072, Avg. Accuracy: 0.9306, Avg. Dice: 0.9431: 100%|| 65/65 [01:07<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9431, Loss:  1.0072\n",
      "Epoch 12 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0173, Avg. Accuracy: 0.9340, Avg. Dice: 0.9456: 100%|| 519/519 [14:41<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9819, Avg. Accuracy: 0.9348, Avg. Dice: 0.9459: 100%|| 65/65 [01:07<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9459, Loss:  0.9819\n",
      "Epoch 13 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0319, Avg. Accuracy: 0.9310, Avg. Dice: 0.9427: 100%|| 519/519 [14:37<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0125, Avg. Accuracy: 0.9276, Avg. Dice: 0.9397: 100%|| 65/65 [01:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9397, Loss:  1.0125\n",
      "Epoch 14 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0105, Avg. Accuracy: 0.9343, Avg. Dice: 0.9457: 100%|| 519/519 [14:33<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0023, Avg. Accuracy: 0.9309, Avg. Dice: 0.9444: 100%|| 65/65 [01:09<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9444, Loss:  1.0023\n",
      "Epoch 15 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9953, Avg. Accuracy: 0.9360, Avg. Dice: 0.9466: 100%|| 519/519 [14:42<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9976, Avg. Accuracy: 0.9310, Avg. Dice: 0.9442: 100%|| 65/65 [01:07<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9442, Loss:  0.9976\n",
      "Epoch 16 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9786, Avg. Accuracy: 0.9388, Avg. Dice: 0.9496: 100%|| 519/519 [14:46<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9817, Avg. Accuracy: 0.9342, Avg. Dice: 0.9453: 100%|| 65/65 [01:07<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9453, Loss:  0.9817\n",
      "Epoch 17 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9693, Avg. Accuracy: 0.9398, Avg. Dice: 0.9503: 100%|| 519/519 [14:38<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9756, Avg. Accuracy: 0.9332, Avg. Dice: 0.9451: 100%|| 65/65 [01:07<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9451, Loss:  0.9756\n",
      "Epoch 18 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9632, Avg. Accuracy: 0.9417, Avg. Dice: 0.9521: 100%|| 519/519 [14:42<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9804, Avg. Accuracy: 0.9339, Avg. Dice: 0.9458: 100%|| 65/65 [01:07<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9458, Loss:  0.9804\n",
      "Epoch 19 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9693, Avg. Accuracy: 0.9397, Avg. Dice: 0.9504: 100%|| 519/519 [14:38<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9639, Avg. Accuracy: 0.9364, Avg. Dice: 0.9477: 100%|| 65/65 [01:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9477, Loss:  0.9639\n",
      "Epoch 20 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9644, Avg. Accuracy: 0.9404, Avg. Dice: 0.9506: 100%|| 519/519 [14:42<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9942, Avg. Accuracy: 0.9297, Avg. Dice: 0.9409: 100%|| 65/65 [01:06<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9409, Loss:  0.9942\n",
      "Epoch 21 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9439, Avg. Accuracy: 0.9431, Avg. Dice: 0.9534: 100%|| 519/519 [14:28<00:00,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9721, Avg. Accuracy: 0.9338, Avg. Dice: 0.9454: 100%|| 65/65 [01:07<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9454, Loss:  0.9721\n",
      "Epoch 22 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9323, Avg. Accuracy: 0.9454, Avg. Dice: 0.9555: 100%|| 519/519 [14:37<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9882, Avg. Accuracy: 0.9301, Avg. Dice: 0.9429: 100%|| 65/65 [01:07<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9429, Loss:  0.9882\n",
      "Epoch 23 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9260, Avg. Accuracy: 0.9457, Avg. Dice: 0.9550: 100%|| 519/519 [14:41<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9744, Avg. Accuracy: 0.9356, Avg. Dice: 0.9469: 100%|| 65/65 [01:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9469, Loss:  0.9744\n",
      "Epoch 24 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9207, Avg. Accuracy: 0.9470, Avg. Dice: 0.9566: 100%|| 519/519 [14:41<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9724, Avg. Accuracy: 0.9338, Avg. Dice: 0.9467: 100%|| 65/65 [01:06<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9467, Loss:  0.9724\n",
      "Epoch 25 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9027, Avg. Accuracy: 0.9492, Avg. Dice: 0.9581: 100%|| 519/519 [14:41<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0021, Avg. Accuracy: 0.9311, Avg. Dice: 0.9438: 100%|| 65/65 [01:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9438, Loss:  1.0021\n",
      "Epoch 26 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9062, Avg. Accuracy: 0.9486, Avg. Dice: 0.9581: 100%|| 519/519 [14:43<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9921, Avg. Accuracy: 0.9331, Avg. Dice: 0.9456: 100%|| 65/65 [01:07<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9456, Loss:  0.9921\n",
      "Epoch 27 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9024, Avg. Accuracy: 0.9491, Avg. Dice: 0.9579: 100%|| 519/519 [14:50<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9773, Avg. Accuracy: 0.9352, Avg. Dice: 0.9467: 100%|| 65/65 [01:07<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9467, Loss:  0.9773\n",
      "Epoch 28 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.8920, Avg. Accuracy: 0.9507, Avg. Dice: 0.9599: 100%|| 519/519 [14:47<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9875, Avg. Accuracy: 0.9312, Avg. Dice: 0.9440: 100%|| 65/65 [01:09<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9440, Loss:  0.9875\n",
      "Epoch 29 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.8780, Avg. Accuracy: 0.9528, Avg. Dice: 0.9614: 100%|| 519/519 [14:51<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9804, Avg. Accuracy: 0.9330, Avg. Dice: 0.9458: 100%|| 65/65 [01:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9458, Loss:  0.9804\n",
      "Epoch 30 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.8661, Avg. Accuracy: 0.9546, Avg. Dice: 0.9631: 100%|| 519/519 [14:54<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9759, Avg. Accuracy: 0.9342, Avg. Dice: 0.9463: 100%|| 65/65 [01:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9463, Loss:  0.9759\n",
      "Test Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Dataset: 100%|| 259/259 [01:13<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8795878215739864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Dataset: 100%|| 259/259 [01:10<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9317323541222072\n",
      "Run 2 ===========\n",
      "Epoch 1 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.8621, Avg. Accuracy: 0.8632, Avg. Dice: 0.8848: 100%|| 519/519 [14:48<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.4784, Avg. Accuracy: 0.9150, Avg. Dice: 0.9304: 100%|| 65/65 [01:08<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9304, Loss:  1.4784\n",
      "Epoch 2 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.4782, Avg. Accuracy: 0.9029, Avg. Dice: 0.9188: 100%|| 519/519 [14:49<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.2090, Avg. Accuracy: 0.9131, Avg. Dice: 0.9264: 100%|| 65/65 [01:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9264, Loss:  1.2090\n",
      "Epoch 3 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.3037, Avg. Accuracy: 0.9120, Avg. Dice: 0.9268: 100%|| 519/519 [14:49<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.1235, Avg. Accuracy: 0.9262, Avg. Dice: 0.9389: 100%|| 65/65 [01:08<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9389, Loss:  1.1235\n",
      "Epoch 4 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.2101, Avg. Accuracy: 0.9172, Avg. Dice: 0.9315: 100%|| 519/519 [14:46<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0695, Avg. Accuracy: 0.9324, Avg. Dice: 0.9447: 100%|| 65/65 [01:09<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9447, Loss:  1.0695\n",
      "Epoch 5 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.1681, Avg. Accuracy: 0.9206, Avg. Dice: 0.9344: 100%|| 519/519 [14:48<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0859, Avg. Accuracy: 0.9174, Avg. Dice: 0.9292: 100%|| 65/65 [01:09<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9292, Loss:  1.0859\n",
      "Epoch 6 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.1108, Avg. Accuracy: 0.9242, Avg. Dice: 0.9374: 100%|| 519/519 [14:51<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0913, Avg. Accuracy: 0.9223, Avg. Dice: 0.9355: 100%|| 65/65 [01:09<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9355, Loss:  1.0913\n",
      "Epoch 7 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.1040, Avg. Accuracy: 0.9244, Avg. Dice: 0.9369: 100%|| 519/519 [14:50<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0577, Avg. Accuracy: 0.9210, Avg. Dice: 0.9340: 100%|| 65/65 [01:08<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9340, Loss:  1.0577\n",
      "Epoch 8 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0771, Avg. Accuracy: 0.9255, Avg. Dice: 0.9385: 100%|| 519/519 [14:52<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9915, Avg. Accuracy: 0.9326, Avg. Dice: 0.9446: 100%|| 65/65 [01:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9446, Loss:  0.9915\n",
      "Epoch 9 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0596, Avg. Accuracy: 0.9283, Avg. Dice: 0.9401: 100%|| 519/519 [14:49<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0047, Avg. Accuracy: 0.9345, Avg. Dice: 0.9465: 100%|| 65/65 [01:08<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9465, Loss:  1.0047\n",
      "Epoch 10 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0606, Avg. Accuracy: 0.9271, Avg. Dice: 0.9400: 100%|| 519/519 [14:52<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0029, Avg. Accuracy: 0.9285, Avg. Dice: 0.9403: 100%|| 65/65 [01:09<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9403, Loss:  1.0029\n",
      "Epoch 11 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0337, Avg. Accuracy: 0.9318, Avg. Dice: 0.9438: 100%|| 519/519 [14:51<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9886, Avg. Accuracy: 0.9331, Avg. Dice: 0.9446: 100%|| 65/65 [01:09<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9446, Loss:  0.9886\n",
      "Epoch 12 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0231, Avg. Accuracy: 0.9326, Avg. Dice: 0.9440: 100%|| 519/519 [14:51<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0050, Avg. Accuracy: 0.9276, Avg. Dice: 0.9399: 100%|| 65/65 [01:08<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9399, Loss:  1.0050\n",
      "Epoch 13 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0241, Avg. Accuracy: 0.9309, Avg. Dice: 0.9427: 100%|| 519/519 [14:52<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9955, Avg. Accuracy: 0.9309, Avg. Dice: 0.9422: 100%|| 65/65 [01:07<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9422, Loss:  0.9955\n",
      "Epoch 14 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9970, Avg. Accuracy: 0.9365, Avg. Dice: 0.9473: 100%|| 519/519 [14:51<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9875, Avg. Accuracy: 0.9331, Avg. Dice: 0.9448: 100%|| 65/65 [01:08<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9448, Loss:  0.9875\n",
      "Epoch 15 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9957, Avg. Accuracy: 0.9346, Avg. Dice: 0.9455: 100%|| 519/519 [14:51<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9821, Avg. Accuracy: 0.9327, Avg. Dice: 0.9438: 100%|| 65/65 [01:08<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9438, Loss:  0.9821\n",
      "Epoch 16 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9802, Avg. Accuracy: 0.9393, Avg. Dice: 0.9502: 100%|| 519/519 [14:50<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9929, Avg. Accuracy: 0.9291, Avg. Dice: 0.9416: 100%|| 65/65 [01:07<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9416, Loss:  0.9929\n",
      "Epoch 17 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0002, Avg. Accuracy: 0.9354, Avg. Dice: 0.9470: 100%|| 519/519 [14:51<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9884, Avg. Accuracy: 0.9306, Avg. Dice: 0.9430: 100%|| 65/65 [01:09<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9430, Loss:  0.9884\n",
      "Epoch 18 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9725, Avg. Accuracy: 0.9390, Avg. Dice: 0.9498: 100%|| 519/519 [14:51<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9923, Avg. Accuracy: 0.9320, Avg. Dice: 0.9434: 100%|| 65/65 [01:08<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9434, Loss:  0.9923\n",
      "Epoch 19 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9620, Avg. Accuracy: 0.9410, Avg. Dice: 0.9513: 100%|| 519/519 [14:51<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9801, Avg. Accuracy: 0.9321, Avg. Dice: 0.9428: 100%|| 65/65 [01:08<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9428, Loss:  0.9801\n",
      "Epoch 20 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9431, Avg. Accuracy: 0.9443, Avg. Dice: 0.9546: 100%|| 519/519 [14:52<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9819, Avg. Accuracy: 0.9323, Avg. Dice: 0.9443: 100%|| 65/65 [01:08<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9443, Loss:  0.9819\n",
      "Epoch 21 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9373, Avg. Accuracy: 0.9443, Avg. Dice: 0.9544: 100%|| 519/519 [14:53<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9821, Avg. Accuracy: 0.9327, Avg. Dice: 0.9428: 100%|| 65/65 [01:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9428, Loss:  0.9821\n",
      "Epoch 22 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9367, Avg. Accuracy: 0.9446, Avg. Dice: 0.9542: 100%|| 519/519 [14:53<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9858, Avg. Accuracy: 0.9329, Avg. Dice: 0.9458: 100%|| 65/65 [01:09<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9458, Loss:  0.9858\n",
      "Epoch 23 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9540, Avg. Accuracy: 0.9420, Avg. Dice: 0.9523: 100%|| 519/519 [14:56<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9938, Avg. Accuracy: 0.9272, Avg. Dice: 0.9376: 100%|| 65/65 [01:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9376, Loss:  0.9938\n",
      "Epoch 24 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9441, Avg. Accuracy: 0.9429, Avg. Dice: 0.9528: 100%|| 519/519 [14:54<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9900, Avg. Accuracy: 0.9301, Avg. Dice: 0.9402: 100%|| 65/65 [01:08<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9402, Loss:  0.9900\n",
      "Epoch 25 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9099, Avg. Accuracy: 0.9489, Avg. Dice: 0.9583: 100%|| 519/519 [15:01<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9785, Avg. Accuracy: 0.9341, Avg. Dice: 0.9446: 100%|| 65/65 [01:09<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9446, Loss:  0.9785\n",
      "Epoch 26 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.8957, Avg. Accuracy: 0.9502, Avg. Dice: 0.9592: 100%|| 519/519 [14:54<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9836, Avg. Accuracy: 0.9349, Avg. Dice: 0.9467: 100%|| 65/65 [01:10<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9467, Loss:  0.9836\n",
      "Epoch 27 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.8987, Avg. Accuracy: 0.9503, Avg. Dice: 0.9590: 100%|| 519/519 [14:56<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0160, Avg. Accuracy: 0.9263, Avg. Dice: 0.9394: 100%|| 65/65 [01:07<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9394, Loss:  1.0160\n",
      "Epoch 28 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.8931, Avg. Accuracy: 0.9503, Avg. Dice: 0.9589: 100%|| 519/519 [14:58<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0028, Avg. Accuracy: 0.9315, Avg. Dice: 0.9447: 100%|| 65/65 [01:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9447, Loss:  1.0028\n",
      "Epoch 29 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.8861, Avg. Accuracy: 0.9515, Avg. Dice: 0.9596: 100%|| 519/519 [14:54<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0004, Avg. Accuracy: 0.9298, Avg. Dice: 0.9401: 100%|| 65/65 [01:08<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9401, Loss:  1.0004\n",
      "Epoch 30 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.8734, Avg. Accuracy: 0.9530, Avg. Dice: 0.9614: 100%|| 519/519 [14:57<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0199, Avg. Accuracy: 0.9294, Avg. Dice: 0.9413: 100%|| 65/65 [01:08<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9413, Loss:  1.0199\n",
      "Test Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Dataset: 100%|| 259/259 [01:13<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8756639809712633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Dataset: 100%|| 259/259 [01:10<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.929883457639348\n",
      "Run 3 ===========\n",
      "Epoch 1 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.8520, Avg. Accuracy: 0.8544, Avg. Dice: 0.8778: 100%|| 519/519 [14:57<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.7873, Avg. Accuracy: 0.8608, Avg. Dice: 0.8873: 100%|| 65/65 [01:08<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.8873, Loss:  1.7873\n",
      "Epoch 2 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.4885, Avg. Accuracy: 0.8944, Avg. Dice: 0.9112: 100%|| 519/519 [15:00<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.2147, Avg. Accuracy: 0.9238, Avg. Dice: 0.9376: 100%|| 65/65 [01:09<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9376, Loss:  1.2147\n",
      "Epoch 3 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.2998, Avg. Accuracy: 0.9097, Avg. Dice: 0.9252: 100%|| 519/519 [14:56<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.1446, Avg. Accuracy: 0.9216, Avg. Dice: 0.9360: 100%|| 65/65 [01:09<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9360, Loss:  1.1446\n",
      "Epoch 4 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.2161, Avg. Accuracy: 0.9158, Avg. Dice: 0.9303: 100%|| 519/519 [14:52<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0898, Avg. Accuracy: 0.9283, Avg. Dice: 0.9410: 100%|| 65/65 [01:08<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9410, Loss:  1.0898\n",
      "Epoch 5 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.1667, Avg. Accuracy: 0.9176, Avg. Dice: 0.9315: 100%|| 519/519 [14:58<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0772, Avg. Accuracy: 0.9243, Avg. Dice: 0.9382: 100%|| 65/65 [01:10<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9382, Loss:  1.0772\n",
      "Epoch 6 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.1256, Avg. Accuracy: 0.9217, Avg. Dice: 0.9350: 100%|| 519/519 [14:54<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0174, Avg. Accuracy: 0.9316, Avg. Dice: 0.9434: 100%|| 65/65 [01:08<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9434, Loss:  1.0174\n",
      "Epoch 7 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.1010, Avg. Accuracy: 0.9236, Avg. Dice: 0.9368: 100%|| 519/519 [14:59<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0261, Avg. Accuracy: 0.9325, Avg. Dice: 0.9457: 100%|| 65/65 [01:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9457, Loss:  1.0261\n",
      "Epoch 8 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.1237, Avg. Accuracy: 0.9180, Avg. Dice: 0.9315: 100%|| 519/519 [14:50<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0490, Avg. Accuracy: 0.9227, Avg. Dice: 0.9359: 100%|| 65/65 [01:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9359, Loss:  1.0490\n",
      "Epoch 9 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0816, Avg. Accuracy: 0.9239, Avg. Dice: 0.9373: 100%|| 519/519 [14:59<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0613, Avg. Accuracy: 0.9169, Avg. Dice: 0.9326: 100%|| 65/65 [01:09<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9326, Loss:  1.0613\n",
      "Epoch 10 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0565, Avg. Accuracy: 0.9284, Avg. Dice: 0.9411: 100%|| 519/519 [14:58<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0059, Avg. Accuracy: 0.9299, Avg. Dice: 0.9434: 100%|| 65/65 [01:09<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9434, Loss:  1.0059\n",
      "Epoch 11 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0505, Avg. Accuracy: 0.9268, Avg. Dice: 0.9395: 100%|| 519/519 [14:53<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0325, Avg. Accuracy: 0.9233, Avg. Dice: 0.9370: 100%|| 65/65 [01:08<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9370, Loss:  1.0325\n",
      "Epoch 12 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0294, Avg. Accuracy: 0.9315, Avg. Dice: 0.9434: 100%|| 519/519 [14:58<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0134, Avg. Accuracy: 0.9303, Avg. Dice: 0.9425: 100%|| 65/65 [01:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9425, Loss:  1.0134\n",
      "Epoch 13 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0226, Avg. Accuracy: 0.9322, Avg. Dice: 0.9440: 100%|| 519/519 [14:52<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0113, Avg. Accuracy: 0.9242, Avg. Dice: 0.9364: 100%|| 65/65 [01:09<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9364, Loss:  1.0113\n",
      "Epoch 14 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0209, Avg. Accuracy: 0.9319, Avg. Dice: 0.9433: 100%|| 519/519 [14:53<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9995, Avg. Accuracy: 0.9292, Avg. Dice: 0.9416: 100%|| 65/65 [01:09<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9416, Loss:  0.9995\n",
      "Epoch 15 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9982, Avg. Accuracy: 0.9363, Avg. Dice: 0.9474: 100%|| 519/519 [14:58<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9929, Avg. Accuracy: 0.9327, Avg. Dice: 0.9450: 100%|| 65/65 [01:08<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9450, Loss:  0.9929\n",
      "Epoch 16 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9869, Avg. Accuracy: 0.9367, Avg. Dice: 0.9477: 100%|| 519/519 [14:55<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9856, Avg. Accuracy: 0.9351, Avg. Dice: 0.9470: 100%|| 65/65 [01:09<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9470, Loss:  0.9856\n",
      "Epoch 17 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9793, Avg. Accuracy: 0.9384, Avg. Dice: 0.9492: 100%|| 519/519 [14:52<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9991, Avg. Accuracy: 0.9299, Avg. Dice: 0.9432: 100%|| 65/65 [01:10<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9432, Loss:  0.9991\n",
      "Epoch 18 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9743, Avg. Accuracy: 0.9390, Avg. Dice: 0.9502: 100%|| 519/519 [14:58<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0031, Avg. Accuracy: 0.9300, Avg. Dice: 0.9415: 100%|| 65/65 [01:07<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9415, Loss:  1.0031\n",
      "Epoch 19 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9539, Avg. Accuracy: 0.9428, Avg. Dice: 0.9531: 100%|| 519/519 [14:55<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9810, Avg. Accuracy: 0.9348, Avg. Dice: 0.9472: 100%|| 65/65 [01:11<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9472, Loss:  0.9810\n",
      "Epoch 20 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9489, Avg. Accuracy: 0.9427, Avg. Dice: 0.9528: 100%|| 519/519 [15:04<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0141, Avg. Accuracy: 0.9286, Avg. Dice: 0.9407: 100%|| 65/65 [01:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9407, Loss:  1.0141\n",
      "Epoch 21 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9451, Avg. Accuracy: 0.9429, Avg. Dice: 0.9531: 100%|| 519/519 [14:57<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9943, Avg. Accuracy: 0.9301, Avg. Dice: 0.9427: 100%|| 65/65 [01:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9427, Loss:  0.9943\n",
      "Epoch 22 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9460, Avg. Accuracy: 0.9430, Avg. Dice: 0.9532: 100%|| 519/519 [14:58<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9963, Avg. Accuracy: 0.9311, Avg. Dice: 0.9430: 100%|| 65/65 [01:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9430, Loss:  0.9963\n",
      "Epoch 23 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9289, Avg. Accuracy: 0.9453, Avg. Dice: 0.9551: 100%|| 519/519 [14:57<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0063, Avg. Accuracy: 0.9304, Avg. Dice: 0.9426: 100%|| 65/65 [01:09<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9426, Loss:  1.0063\n",
      "Epoch 24 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9191, Avg. Accuracy: 0.9471, Avg. Dice: 0.9563: 100%|| 519/519 [14:56<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9956, Avg. Accuracy: 0.9314, Avg. Dice: 0.9447: 100%|| 65/65 [01:09<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9447, Loss:  0.9956\n",
      "Epoch 25 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9112, Avg. Accuracy: 0.9478, Avg. Dice: 0.9570: 100%|| 519/519 [14:58<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9807, Avg. Accuracy: 0.9305, Avg. Dice: 0.9415: 100%|| 65/65 [01:09<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9415, Loss:  0.9807\n",
      "Epoch 26 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9292, Avg. Accuracy: 0.9449, Avg. Dice: 0.9551: 100%|| 519/519 [15:01<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9949, Avg. Accuracy: 0.9290, Avg. Dice: 0.9419: 100%|| 65/65 [01:09<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9419, Loss:  0.9949\n",
      "Epoch 27 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9129, Avg. Accuracy: 0.9472, Avg. Dice: 0.9563: 100%|| 519/519 [14:57<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0082, Avg. Accuracy: 0.9311, Avg. Dice: 0.9433: 100%|| 65/65 [01:09<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9433, Loss:  1.0082\n",
      "Epoch 28 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.8889, Avg. Accuracy: 0.9510, Avg. Dice: 0.9596: 100%|| 519/519 [14:59<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9960, Avg. Accuracy: 0.9336, Avg. Dice: 0.9458: 100%|| 65/65 [01:10<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9458, Loss:  0.9960\n",
      "Epoch 29 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.8986, Avg. Accuracy: 0.9501, Avg. Dice: 0.9590: 100%|| 519/519 [14:59<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9765, Avg. Accuracy: 0.9319, Avg. Dice: 0.9430: 100%|| 65/65 [01:09<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9430, Loss:  0.9765\n",
      "Epoch 30 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.8837, Avg. Accuracy: 0.9516, Avg. Dice: 0.9600: 100%|| 519/519 [15:02<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9936, Avg. Accuracy: 0.9327, Avg. Dice: 0.9453: 100%|| 65/65 [01:09<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9453, Loss:  0.9936\n",
      "Test Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Dataset: 100%|| 259/259 [01:13<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8723782692026347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Dataset: 100%|| 259/259 [01:11<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9265365035215368\n",
      "Run 4 ===========\n",
      "Epoch 1 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.9541, Avg. Accuracy: 0.8479, Avg. Dice: 0.8744: 100%|| 519/519 [14:53<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.5774, Avg. Accuracy: 0.9091, Avg. Dice: 0.9254: 100%|| 65/65 [01:09<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9254, Loss:  1.5774\n",
      "Epoch 2 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.5638, Avg. Accuracy: 0.8967, Avg. Dice: 0.9139: 100%|| 519/519 [15:01<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.3500, Avg. Accuracy: 0.9008, Avg. Dice: 0.9161: 100%|| 65/65 [01:08<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9161, Loss:  1.3500\n",
      "Epoch 3 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.3524, Avg. Accuracy: 0.9091, Avg. Dice: 0.9247: 100%|| 519/519 [14:56<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.1984, Avg. Accuracy: 0.9187, Avg. Dice: 0.9328: 100%|| 65/65 [01:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9328, Loss:  1.1984\n",
      "Epoch 4 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.2410, Avg. Accuracy: 0.9154, Avg. Dice: 0.9295: 100%|| 519/519 [15:00<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.1220, Avg. Accuracy: 0.9214, Avg. Dice: 0.9334: 100%|| 65/65 [01:09<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9334, Loss:  1.1220\n",
      "Epoch 5 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.1789, Avg. Accuracy: 0.9195, Avg. Dice: 0.9334: 100%|| 519/519 [14:59<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0747, Avg. Accuracy: 0.9255, Avg. Dice: 0.9367: 100%|| 65/65 [01:09<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9367, Loss:  1.0747\n",
      "Epoch 6 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.1322, Avg. Accuracy: 0.9221, Avg. Dice: 0.9356: 100%|| 519/519 [15:02<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0586, Avg. Accuracy: 0.9242, Avg. Dice: 0.9361: 100%|| 65/65 [01:09<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9361, Loss:  1.0586\n",
      "Epoch 7 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.1065, Avg. Accuracy: 0.9245, Avg. Dice: 0.9374: 100%|| 519/519 [14:53<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0325, Avg. Accuracy: 0.9262, Avg. Dice: 0.9384: 100%|| 65/65 [01:09<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9384, Loss:  1.0325\n",
      "Epoch 8 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0900, Avg. Accuracy: 0.9247, Avg. Dice: 0.9375: 100%|| 519/519 [15:01<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0550, Avg. Accuracy: 0.9271, Avg. Dice: 0.9392: 100%|| 65/65 [01:08<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9392, Loss:  1.0550\n",
      "Epoch 9 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0666, Avg. Accuracy: 0.9288, Avg. Dice: 0.9415: 100%|| 519/519 [14:59<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0057, Avg. Accuracy: 0.9316, Avg. Dice: 0.9442: 100%|| 65/65 [01:09<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9442, Loss:  1.0057\n",
      "Epoch 10 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0392, Avg. Accuracy: 0.9320, Avg. Dice: 0.9442: 100%|| 519/519 [15:04<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9986, Avg. Accuracy: 0.9303, Avg. Dice: 0.9420: 100%|| 65/65 [01:09<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9420, Loss:  0.9986\n",
      "Epoch 11 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0523, Avg. Accuracy: 0.9279, Avg. Dice: 0.9400: 100%|| 519/519 [15:00<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0108, Avg. Accuracy: 0.9305, Avg. Dice: 0.9422: 100%|| 65/65 [01:09<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9422, Loss:  1.0108\n",
      "Epoch 12 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0260, Avg. Accuracy: 0.9312, Avg. Dice: 0.9425: 100%|| 519/519 [15:03<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9921, Avg. Accuracy: 0.9328, Avg. Dice: 0.9451: 100%|| 65/65 [01:08<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9451, Loss:  0.9921\n",
      "Epoch 13 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0076, Avg. Accuracy: 0.9351, Avg. Dice: 0.9460: 100%|| 519/519 [14:57<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0108, Avg. Accuracy: 0.9275, Avg. Dice: 0.9393: 100%|| 65/65 [01:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9393, Loss:  1.0108\n",
      "Epoch 14 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9993, Avg. Accuracy: 0.9363, Avg. Dice: 0.9469: 100%|| 519/519 [14:58<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9892, Avg. Accuracy: 0.9310, Avg. Dice: 0.9416: 100%|| 65/65 [01:09<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9416, Loss:  0.9892\n",
      "Epoch 15 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9891, Avg. Accuracy: 0.9371, Avg. Dice: 0.9476: 100%|| 519/519 [14:58<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9623, Avg. Accuracy: 0.9383, Avg. Dice: 0.9498: 100%|| 65/65 [01:09<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9498, Loss:  0.9623\n",
      "Epoch 16 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9842, Avg. Accuracy: 0.9380, Avg. Dice: 0.9490: 100%|| 519/519 [15:03<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9856, Avg. Accuracy: 0.9328, Avg. Dice: 0.9437: 100%|| 65/65 [01:09<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9437, Loss:  0.9856\n",
      "Epoch 17 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9660, Avg. Accuracy: 0.9409, Avg. Dice: 0.9513: 100%|| 519/519 [15:03<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0127, Avg. Accuracy: 0.9262, Avg. Dice: 0.9366: 100%|| 65/65 [01:08<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9366, Loss:  1.0127\n",
      "Epoch 18 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9676, Avg. Accuracy: 0.9403, Avg. Dice: 0.9504: 100%|| 519/519 [14:59<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9846, Avg. Accuracy: 0.9339, Avg. Dice: 0.9454: 100%|| 65/65 [01:08<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9454, Loss:  0.9846\n",
      "Epoch 19 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9654, Avg. Accuracy: 0.9398, Avg. Dice: 0.9505: 100%|| 519/519 [15:05<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9860, Avg. Accuracy: 0.9340, Avg. Dice: 0.9461: 100%|| 65/65 [01:09<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9461, Loss:  0.9860\n",
      "Epoch 20 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9685, Avg. Accuracy: 0.9393, Avg. Dice: 0.9505: 100%|| 519/519 [15:11<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9933, Avg. Accuracy: 0.9309, Avg. Dice: 0.9442: 100%|| 65/65 [01:09<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9442, Loss:  0.9933\n",
      "Epoch 21 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9397, Avg. Accuracy: 0.9441, Avg. Dice: 0.9540: 100%|| 519/519 [15:15<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0009, Avg. Accuracy: 0.9275, Avg. Dice: 0.9389: 100%|| 65/65 [01:09<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9389, Loss:  1.0009\n",
      "Epoch 22 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9312, Avg. Accuracy: 0.9461, Avg. Dice: 0.9559: 100%|| 519/519 [15:12<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0070, Avg. Accuracy: 0.9280, Avg. Dice: 0.9406: 100%|| 65/65 [01:09<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9406, Loss:  1.0070\n",
      "Epoch 23 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9236, Avg. Accuracy: 0.9468, Avg. Dice: 0.9565: 100%|| 519/519 [15:22<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9807, Avg. Accuracy: 0.9339, Avg. Dice: 0.9456: 100%|| 65/65 [01:10<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9456, Loss:  0.9807\n",
      "Epoch 24 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9165, Avg. Accuracy: 0.9466, Avg. Dice: 0.9560: 100%|| 519/519 [15:15<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9941, Avg. Accuracy: 0.9320, Avg. Dice: 0.9430: 100%|| 65/65 [01:10<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9430, Loss:  0.9941\n",
      "Epoch 25 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9011, Avg. Accuracy: 0.9495, Avg. Dice: 0.9584: 100%|| 519/519 [15:19<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0038, Avg. Accuracy: 0.9274, Avg. Dice: 0.9392: 100%|| 65/65 [01:09<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9392, Loss:  1.0038\n",
      "Epoch 26 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9024, Avg. Accuracy: 0.9499, Avg. Dice: 0.9592: 100%|| 519/519 [15:02<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9988, Avg. Accuracy: 0.9274, Avg. Dice: 0.9390: 100%|| 65/65 [01:10<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9390, Loss:  0.9988\n",
      "Epoch 27 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9022, Avg. Accuracy: 0.9498, Avg. Dice: 0.9588: 100%|| 519/519 [15:16<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9901, Avg. Accuracy: 0.9299, Avg. Dice: 0.9429: 100%|| 65/65 [01:10<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9429, Loss:  0.9901\n",
      "Epoch 28 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.8896, Avg. Accuracy: 0.9513, Avg. Dice: 0.9603: 100%|| 519/519 [15:09<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0187, Avg. Accuracy: 0.9284, Avg. Dice: 0.9405: 100%|| 65/65 [01:10<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9405, Loss:  1.0187\n",
      "Epoch 29 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.8887, Avg. Accuracy: 0.9508, Avg. Dice: 0.9593: 100%|| 519/519 [15:07<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9722, Avg. Accuracy: 0.9329, Avg. Dice: 0.9445: 100%|| 65/65 [01:10<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9445, Loss:  0.9722\n",
      "Epoch 30 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.8688, Avg. Accuracy: 0.9538, Avg. Dice: 0.9620: 100%|| 519/519 [15:05<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0165, Avg. Accuracy: 0.9288, Avg. Dice: 0.9416: 100%|| 65/65 [01:09<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9416, Loss:  1.0165\n",
      "Test Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Dataset: 100%|| 259/259 [01:13<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8680565129142223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Dataset: 100%|| 259/259 [01:12<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9258401753820115\n",
      "Run 5 ===========\n",
      "Epoch 1 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.8017, Avg. Accuracy: 0.8651, Avg. Dice: 0.8867: 100%|| 519/519 [14:57<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.4089, Avg. Accuracy: 0.9099, Avg. Dice: 0.9260: 100%|| 65/65 [01:09<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9260, Loss:  1.4089\n",
      "Epoch 2 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.4525, Avg. Accuracy: 0.9009, Avg. Dice: 0.9164: 100%|| 519/519 [15:01<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.2252, Avg. Accuracy: 0.9172, Avg. Dice: 0.9322: 100%|| 65/65 [01:08<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9322, Loss:  1.2252\n",
      "Epoch 3 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.2808, Avg. Accuracy: 0.9124, Avg. Dice: 0.9273: 100%|| 519/519 [15:03<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.1088, Avg. Accuracy: 0.9235, Avg. Dice: 0.9378: 100%|| 65/65 [01:09<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9378, Loss:  1.1088\n",
      "Epoch 4 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.2075, Avg. Accuracy: 0.9164, Avg. Dice: 0.9307: 100%|| 519/519 [14:53<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0925, Avg. Accuracy: 0.9213, Avg. Dice: 0.9342: 100%|| 65/65 [01:09<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9342, Loss:  1.0925\n",
      "Epoch 5 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.1635, Avg. Accuracy: 0.9196, Avg. Dice: 0.9339: 100%|| 519/519 [14:56<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0380, Avg. Accuracy: 0.9293, Avg. Dice: 0.9419: 100%|| 65/65 [01:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9419, Loss:  1.0380\n",
      "Epoch 6 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.1845, Avg. Accuracy: 0.9123, Avg. Dice: 0.9271: 100%|| 519/519 [14:59<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0772, Avg. Accuracy: 0.9203, Avg. Dice: 0.9333: 100%|| 65/65 [01:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9333, Loss:  1.0772\n",
      "Epoch 7 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.1233, Avg. Accuracy: 0.9210, Avg. Dice: 0.9348: 100%|| 519/519 [14:58<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0319, Avg. Accuracy: 0.9273, Avg. Dice: 0.9376: 100%|| 65/65 [01:09<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9376, Loss:  1.0319\n",
      "Epoch 8 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0897, Avg. Accuracy: 0.9245, Avg. Dice: 0.9373: 100%|| 519/519 [14:58<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0591, Avg. Accuracy: 0.9257, Avg. Dice: 0.9399: 100%|| 65/65 [01:08<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9399, Loss:  1.0591\n",
      "Epoch 9 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0628, Avg. Accuracy: 0.9289, Avg. Dice: 0.9410: 100%|| 519/519 [14:58<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0104, Avg. Accuracy: 0.9321, Avg. Dice: 0.9438: 100%|| 65/65 [01:07<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9438, Loss:  1.0104\n",
      "Epoch 10 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0558, Avg. Accuracy: 0.9277, Avg. Dice: 0.9401: 100%|| 519/519 [14:55<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0054, Avg. Accuracy: 0.9329, Avg. Dice: 0.9451: 100%|| 65/65 [01:07<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9451, Loss:  1.0054\n",
      "Epoch 11 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0418, Avg. Accuracy: 0.9307, Avg. Dice: 0.9423: 100%|| 519/519 [14:55<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9837, Avg. Accuracy: 0.9340, Avg. Dice: 0.9459: 100%|| 65/65 [01:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9459, Loss:  0.9837\n",
      "Epoch 12 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0384, Avg. Accuracy: 0.9307, Avg. Dice: 0.9426: 100%|| 519/519 [14:57<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0014, Avg. Accuracy: 0.9303, Avg. Dice: 0.9428: 100%|| 65/65 [01:10<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9428, Loss:  1.0014\n",
      "Epoch 13 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0250, Avg. Accuracy: 0.9323, Avg. Dice: 0.9438: 100%|| 519/519 [14:54<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0002, Avg. Accuracy: 0.9307, Avg. Dice: 0.9431: 100%|| 65/65 [01:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9431, Loss:  1.0002\n",
      "Epoch 14 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0115, Avg. Accuracy: 0.9341, Avg. Dice: 0.9450: 100%|| 519/519 [14:51<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9878, Avg. Accuracy: 0.9323, Avg. Dice: 0.9433: 100%|| 65/65 [01:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9433, Loss:  0.9878\n",
      "Epoch 15 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0028, Avg. Accuracy: 0.9364, Avg. Dice: 0.9475: 100%|| 519/519 [14:53<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9914, Avg. Accuracy: 0.9309, Avg. Dice: 0.9443: 100%|| 65/65 [01:07<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9443, Loss:  0.9914\n",
      "Epoch 16 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0030, Avg. Accuracy: 0.9355, Avg. Dice: 0.9469: 100%|| 519/519 [14:52<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0418, Avg. Accuracy: 0.9205, Avg. Dice: 0.9351: 100%|| 65/65 [01:07<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9351, Loss:  1.0418\n",
      "Epoch 17 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9956, Avg. Accuracy: 0.9359, Avg. Dice: 0.9466: 100%|| 519/519 [14:50<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9873, Avg. Accuracy: 0.9309, Avg. Dice: 0.9444: 100%|| 65/65 [01:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9444, Loss:  0.9873\n",
      "Epoch 18 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9729, Avg. Accuracy: 0.9400, Avg. Dice: 0.9507: 100%|| 519/519 [14:50<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9797, Avg. Accuracy: 0.9318, Avg. Dice: 0.9432: 100%|| 65/65 [01:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9432, Loss:  0.9797\n",
      "Epoch 19 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9815, Avg. Accuracy: 0.9380, Avg. Dice: 0.9489: 100%|| 519/519 [14:52<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0022, Avg. Accuracy: 0.9287, Avg. Dice: 0.9421: 100%|| 65/65 [01:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9421, Loss:  1.0022\n",
      "Epoch 20 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9709, Avg. Accuracy: 0.9396, Avg. Dice: 0.9503: 100%|| 519/519 [14:52<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0145, Avg. Accuracy: 0.9222, Avg. Dice: 0.9352: 100%|| 65/65 [01:08<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9352, Loss:  1.0145\n",
      "Epoch 21 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9640, Avg. Accuracy: 0.9401, Avg. Dice: 0.9502: 100%|| 519/519 [14:52<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9813, Avg. Accuracy: 0.9362, Avg. Dice: 0.9478: 100%|| 65/65 [01:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9478, Loss:  0.9813\n",
      "Epoch 22 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9526, Avg. Accuracy: 0.9424, Avg. Dice: 0.9523: 100%|| 519/519 [14:52<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9837, Avg. Accuracy: 0.9333, Avg. Dice: 0.9442: 100%|| 65/65 [01:07<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9442, Loss:  0.9837\n",
      "Epoch 23 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9300, Avg. Accuracy: 0.9467, Avg. Dice: 0.9567: 100%|| 519/519 [14:50<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0030, Avg. Accuracy: 0.9311, Avg. Dice: 0.9434: 100%|| 65/65 [01:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9434, Loss:  1.0030\n",
      "Epoch 24 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9275, Avg. Accuracy: 0.9466, Avg. Dice: 0.9561: 100%|| 519/519 [14:48<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9855, Avg. Accuracy: 0.9354, Avg. Dice: 0.9468: 100%|| 65/65 [01:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9468, Loss:  0.9855\n",
      "Epoch 25 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9134, Avg. Accuracy: 0.9480, Avg. Dice: 0.9574: 100%|| 519/519 [14:50<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9869, Avg. Accuracy: 0.9327, Avg. Dice: 0.9445: 100%|| 65/65 [01:08<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9445, Loss:  0.9869\n",
      "Epoch 26 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9071, Avg. Accuracy: 0.9494, Avg. Dice: 0.9584: 100%|| 519/519 [14:50<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0099, Avg. Accuracy: 0.9302, Avg. Dice: 0.9415: 100%|| 65/65 [01:07<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9415, Loss:  1.0099\n",
      "Epoch 27 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.8949, Avg. Accuracy: 0.9509, Avg. Dice: 0.9601: 100%|| 519/519 [14:48<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 1.0061, Avg. Accuracy: 0.9323, Avg. Dice: 0.9447: 100%|| 65/65 [01:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9447, Loss:  1.0061\n",
      "Epoch 28 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.8967, Avg. Accuracy: 0.9505, Avg. Dice: 0.9596: 100%|| 519/519 [14:49<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9819, Avg. Accuracy: 0.9345, Avg. Dice: 0.9462: 100%|| 65/65 [01:08<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9462, Loss:  0.9819\n",
      "Epoch 29 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9134, Avg. Accuracy: 0.9466, Avg. Dice: 0.9563: 100%|| 519/519 [14:52<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9921, Avg. Accuracy: 0.9333, Avg. Dice: 0.9457: 100%|| 65/65 [01:08<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9457, Loss:  0.9921\n",
      "Epoch 30 ---------------------\n",
      "Training Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.8771, Avg. Accuracy: 0.9528, Avg. Dice: 0.9613: 100%|| 519/519 [14:56<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg. Loss: 0.9786, Avg. Accuracy: 0.9366, Avg. Dice: 0.9488: 100%|| 65/65 [01:10<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice:  0.9488, Loss:  0.9786\n",
      "Test Set -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Dataset: 100%|| 259/259 [01:12<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8692201069820643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Dataset: 100%|| 259/259 [01:11<00:00,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9258816582190842\n",
      "FINAL RESULTS:\n",
      "POLAR=0.9280+/-0.0024\n",
      "POLAR=0.8730+/-0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_pol = []\n",
    "results = []\n",
    "model_name = 'PolyEmbed-FocalStructLossp'\n",
    "for iter in range(1,6):\n",
    "    print(f'Run {iter} ===========')\n",
    "    net = DuAT(dim=32, optimizer=torch.optim.AdamW, learning_rate=1e-4, weight_decay=3e-4, loss_fn=focal_struct_loss)\n",
    "    maxdice = 0.\n",
    "    \n",
    "    for t in range(30):\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"Epoch {t+1} ---------------------\")\n",
    "        print(f\"Training Set -----\")\n",
    "        train_loss, _ = train(net)\n",
    "        print(f\"Validation Set -----\")\n",
    "        test_loss, dice = test(net)\n",
    "        print(f\"\\rDice: {dice: .4f}, Loss: {test_loss : .4f}\")\n",
    "        if dice > maxdice:\n",
    "            maxdice = dice\n",
    "            torch.save(net, f'FinalModels/{model_name}-{iter}.pth')\n",
    "            \n",
    "    print(f\"Test Set -----\")\n",
    "    net = torch.load(f'FinalModels/{model_name}-{iter}.pth')\n",
    "    output(net)\n",
    "    raw_output(net)\n",
    "\n",
    "print('FINAL RESULTS:')\n",
    "print(f'POLAR={np.mean(np.array(results_pol)):.4f}+/-{np.std(np.array(results_pol)):.4f}')\n",
    "print(f'POLAR={np.mean(np.array(results)):.4f}+/-{np.std(np.array(results)):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a7d09c12-1ad0-4981-8efd-4da1e61453bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, mask_l, mask_s, y, filename, center in test_dl:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0d07fa35-c668-4516-aaae-e194849a7667",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1, h2 = net(X, center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5bfde85c-9630-4f22-9533-9bc1b8a3c985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 256, 256])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "87e6edba-e6fc-48ef-adcd-d4496b24b8fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=256x256>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.transforms.ToPILImage()(h1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9f9c0ba5-5898-4daa-9b20-fffa409c8fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=512x384>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.transforms.ToPILImage()(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6daced3-ff57-4886-8e07-2b38fd65ec27",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) :-1: error: (-5:Bad argument) in function 'morphologyEx'\n> Overload resolution failed:\n>  - src is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'src'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mprocess_tensor_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 5\u001b[0m, in \u001b[0;36mprocess_tensor_full\u001b[1;34m(tensor, center)\u001b[0m\n\u001b[0;32m      3\u001b[0m kernel \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m),np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# closing = tensor_np\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m closing \u001b[38;5;241m=\u001b[39m \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmorphologyEx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMORPH_ERODE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m _, labels \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mconnectedComponents(closing)\n\u001b[0;32m      7\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39misin(labels, labels[:, \u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) :-1: error: (-5:Bad argument) in function 'morphologyEx'\n> Overload resolution failed:\n>  - src is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'src'\n"
     ]
    }
   ],
   "source": [
    "process_tensor_full(h2, center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "72e225aa-c9c4-4be6-9014-99cd0568c916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tensor_full(tensor, center):\n",
    "    tensor_np = tensor\n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    # closing = tensor_np\n",
    "    closing = cv.morphologyEx(tensor_np, cv.MORPH_ERODE, kernel)\n",
    "    _, labels = cv.connectedComponents(closing)\n",
    "    mask = np.isin(labels, labels[:, 0])\n",
    "    cleaned = np.where(mask, closing, 0)\n",
    "    # cleaned = closing\n",
    "    cleaned = cv.morphologyEx(cleaned, cv.MORPH_DILATE, kernel)\n",
    "\n",
    "    \n",
    "    \n",
    "    value = np.sqrt(((cleaned.shape[1]/2.0)**2.0)+((cleaned.shape[0]/2.0)**2.0))\n",
    "    \n",
    "    polar_image = cv.linearPolar(cleaned, center, value, cv.WARP_FILL_OUTLIERS + cv.WARP_INVERSE_MAP)\n",
    "    # polar_image = closing\n",
    "    kernel = np.ones((10, 10),np.uint8)\n",
    "\n",
    "    cleaned = cv.morphologyEx(polar_image, cv.MORPH_CLOSE, kernel)\n",
    "    return cleaned\n",
    "\n",
    "def process_tensor_basic(tensor, center):\n",
    "    value = np.sqrt(((tensor.shape[1]/2.0)**2.0)+((tensor.shape[0]/2.0)**2.0))\n",
    "    \n",
    "    polar_image = cv.linearPolar(tensor, center, value, cv.WARP_FILL_OUTLIERS + cv.WARP_INVERSE_MAP)\n",
    "    return polar_image\n",
    "\n",
    "process_tensor = process_tensor_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4d56b2e-a808-48df-a86d-394ea73145e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_INTERM_DIR = 'Final-Improv'\n",
    "\n",
    "def output(model):\n",
    "    num_batches = len(valid_output_dl)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # with tqdm(total=num_batches) as pbar:\n",
    "        #     pbar.set_description(\"Training Dataset\")\n",
    "        #     for (X, mask, y, filename, center) in train_output_dl:\n",
    "        #         center = [c.item() for c in center]\n",
    "        #         h = model(X, mask)\n",
    "        #         h = h.detach().cpu().numpy()[0,0]\n",
    "        #         h = to_cart(h, center)\n",
    "        #         cv.imwrite(os.path.join(TRAIN_INTERM_DIR, filename[0] + '-interm.jpg'), h * 255)\n",
    "        #         pbar.update(1)\n",
    "                \n",
    "        num_batches = len(valid_output_dl)\n",
    "        dices = []\n",
    "        with tqdm(total=num_batches) as pbar:\n",
    "            pbar.set_description(\"Validation Dataset\")\n",
    "            for (X, mask, y, filename, center) in valid_output_dl:\n",
    "                center = [c.item() for c in center]\n",
    "                # h1, h2 = model(X, mask)\n",
    "                X = X.to(DEVICE)\n",
    "                h1, h2 = model(X, mask)\n",
    "                # h2 = model(X, mask)\n",
    "                H = X.shape[2] // 4\n",
    "                # h1 = h1[:,:,H:-H,:]\n",
    "                # h2 = h2[:,:,H:-H,:]\n",
    "                mask = mask.to(DEVICE)\n",
    "                # h = (h1 + h2)*mask / 2\n",
    "                h = h2 # * mask\n",
    "                # print(h)\n",
    "                h = h.detach().cpu().numpy()[0,0]\n",
    "                y_tilde = np.uint8(h > .0)\n",
    "                y_tilde = process_tensor(y_tilde, center)\n",
    "                h = process_tensor(np.uint8(h * 255), center)\n",
    "                y = y.detach().cpu().numpy()[0,0]\n",
    "                y = np.round(y)\n",
    "                tp = np.sum(np.minimum(y_tilde, y))\n",
    "                fp = np.sum(np.minimum(y_tilde, 1 - y))\n",
    "                fn = np.sum(np.minimum(1 - y_tilde, y))\n",
    "                tn = np.sum(np.minimum(1 - y_tilde, 1 - y))\n",
    "                dice = 2 * tp / (2 * tp + fp + fn)\n",
    "                dices.append(dice)\n",
    "                img = np.concatenate([y_tilde * 255, y * 255], axis=1)\n",
    "                # cv.imwrite(os.path.join(VAL_INTERM_DIR, f'{dice:.4f}-' + filename[0] + '.jpg'), img)\n",
    "                pbar.update(1)\n",
    "        print(np.mean(np.array(dices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a33e88fb-2cec-4b68-b487-87c65f66ce6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Dataset: 100%|| 259/259 [00:54<00:00,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8791063990947947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# net.pad = True\n",
    "output(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9b54f5-ea84-4edd-a66e-78a0976c5da5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
